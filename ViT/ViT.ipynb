{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision\nfrom torchvision.transforms import v2\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom random import choices\nimport numpy as np\nimport random\nfrom sklearn.model_selection import train_test_split\nimport warnings\nfrom sklearn.feature_extraction.image import extract_patches_2d","metadata":{"id":"LX1zgjQrJJeI","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:01:42.155803Z","iopub.execute_input":"2025-05-18T16:01:42.156031Z","iopub.status.idle":"2025-05-18T16:01:51.051020Z","shell.execute_reply.started":"2025-05-18T16:01:42.156005Z","shell.execute_reply":"2025-05-18T16:01:51.050259Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nseed = 42\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"AzBP5GV1JM5f","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:01:56.394697Z","iopub.execute_input":"2025-05-18T16:01:56.394970Z","iopub.status.idle":"2025-05-18T16:01:56.481309Z","shell.execute_reply.started":"2025-05-18T16:01:56.394950Z","shell.execute_reply":"2025-05-18T16:01:56.480358Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"torch.random.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)","metadata":{"id":"wd41xS2oJON0","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:01:58.472287Z","iopub.execute_input":"2025-05-18T16:01:58.472967Z","iopub.status.idle":"2025-05-18T16:01:58.484322Z","shell.execute_reply.started":"2025-05-18T16:01:58.472937Z","shell.execute_reply":"2025-05-18T16:01:58.483567Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"training_dataset = torchvision.datasets.CIFAR10(root=\"./\", download=True, train=True)\ntest_dataset = torchvision.datasets.CIFAR10(root=\"./\", download=True, train=False)","metadata":{"id":"PYWG4MXbJQuB","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:02:00.180636Z","iopub.execute_input":"2025-05-18T16:02:00.181425Z","iopub.status.idle":"2025-05-18T16:02:05.086862Z","shell.execute_reply.started":"2025-05-18T16:02:00.181388Z","shell.execute_reply":"2025-05-18T16:02:05.086057Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 170M/170M [00:01<00:00, 105MB/s]  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"training_images = []\ntest_images = []\ntransformation = v2.Compose([\n    v2.ToTensor(),\n    v2.Pad(padding=2)\n\n])\n\nfor img in training_dataset.data:\n  training_images.append(transformation(img))\n\nfor img in test_dataset.data:\n  test_images.append(transformation(img))\n\ntraining_images = torch.stack(training_images)\ntest_images = torch.stack(test_images)","metadata":{"id":"FoxsL91GJZEJ","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:02:05.088258Z","iopub.execute_input":"2025-05-18T16:02:05.088549Z","iopub.status.idle":"2025-05-18T16:02:11.667559Z","shell.execute_reply.started":"2025-05-18T16:02:05.088524Z","shell.execute_reply":"2025-05-18T16:02:11.666689Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(training_images.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h4EM9zPdnUo7","outputId":"44de06dc-a5de-4187-e2e3-1a75ef18a14e","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:02:11.668341Z","iopub.execute_input":"2025-05-18T16:02:11.668613Z","iopub.status.idle":"2025-05-18T16:02:11.673801Z","shell.execute_reply.started":"2025-05-18T16:02:11.668590Z","shell.execute_reply":"2025-05-18T16:02:11.672923Z"}},"outputs":[{"name":"stdout","text":"torch.Size([50000, 3, 36, 36])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"channel_1 = training_images[:, 0, :, :]\nchannel_2 = training_images[:, 1, :, :]\nchannel_3 = training_images[:, 2, :, :]","metadata":{"id":"rII3PTMUnLCu","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:02:11.676054Z","iopub.execute_input":"2025-05-18T16:02:11.676255Z","iopub.status.idle":"2025-05-18T16:02:12.649685Z","shell.execute_reply.started":"2025-05-18T16:02:11.676239Z","shell.execute_reply":"2025-05-18T16:02:12.648833Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(channel_1.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AYE3tcy6n2Hy","outputId":"b7e6bdcd-c14d-49ac-acdf-878217fc3e71","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:02:12.650480Z","iopub.execute_input":"2025-05-18T16:02:12.650718Z","iopub.status.idle":"2025-05-18T16:02:12.664253Z","shell.execute_reply.started":"2025-05-18T16:02:12.650701Z","shell.execute_reply":"2025-05-18T16:02:12.663531Z"}},"outputs":[{"name":"stdout","text":"torch.Size([50000, 36, 36])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# https://stackoverflow.com/a/16858283\ndef blockshaped(arr, nrows, ncols):\n    \"\"\"\n    Return an array of shape (n, nrows, ncols) where\n    n * nrows * ncols = arr.size\n\n    If arr is a 2D array, the returned array should look like n subblocks with\n    each subblock preserving the \"physical\" layout of arr.\n    \"\"\"\n    h, w = arr.shape\n    assert h % nrows == 0, f\"{h} rows is not evenly divisible by {nrows}\"\n    assert w % ncols == 0, f\"{w} cols is not evenly divisible by {ncols}\"\n    return (arr.reshape(h//nrows, nrows, -1, ncols)\n               .swapaxes(1,2)\n               .reshape(-1, nrows, ncols))","metadata":{"id":"mcFLy33Soynn","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:02:12.665087Z","iopub.execute_input":"2025-05-18T16:02:12.665356Z","iopub.status.idle":"2025-05-18T16:02:12.678603Z","shell.execute_reply.started":"2025-05-18T16:02:12.665332Z","shell.execute_reply":"2025-05-18T16:02:12.677926Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\nchannel_1_patches = []\nchannel_2_patches = []\nchannel_3_patches = []\nfor i in range(channel_1.shape[0]):\n  channel_1_patches.append(blockshaped(channel_1[i], 12, 12))\nchannel_1_patches = torch.stack(channel_1_patches)\nfor i in range(channel_2.shape[0]):\n  channel_2_patches.append(blockshaped(channel_2[i], 12, 12))\nchannel_2_patches = torch.stack(channel_2_patches)\nfor i in range(channel_3.shape[0]):\n  channel_3_patches.append(blockshaped(channel_3[i], 12, 12))\nchannel_3_patches = torch.stack(channel_3_patches)","metadata":{"id":"-R6sE0eDnam9","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:02:12.679337Z","iopub.execute_input":"2025-05-18T16:02:12.679733Z","iopub.status.idle":"2025-05-18T16:02:16.145477Z","shell.execute_reply.started":"2025-05-18T16:02:12.679712Z","shell.execute_reply":"2025-05-18T16:02:16.144645Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\npatches = torch.stack([channel_1_patches, channel_2_patches, channel_3_patches], dim=1).permute(0, 2, 1, 3, 4)","metadata":{"id":"cnf2jYBHqMD6","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:02:16.146328Z","iopub.execute_input":"2025-05-18T16:02:16.147267Z","iopub.status.idle":"2025-05-18T16:02:16.472632Z","shell.execute_reply.started":"2025-05-18T16:02:16.147247Z","shell.execute_reply":"2025-05-18T16:02:16.472002Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_labels = training_dataset.targets\ntest_labels = test_dataset.targets\ny_train = torch.Tensor([[1 if i == el else 0 for i in range(10)] for el in train_labels])\ny_test = torch.Tensor([[1 if i == el else 0 for i in range(10)] for el in test_labels])","metadata":{"id":"Oi2MDQXomzef","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:02:16.474154Z","iopub.execute_input":"2025-05-18T16:02:16.474405Z","iopub.status.idle":"2025-05-18T16:02:16.713557Z","shell.execute_reply.started":"2025-05-18T16:02:16.474362Z","shell.execute_reply":"2025-05-18T16:02:16.713020Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\n\nclass MLP(nn.Module):\n\n  def __init__(self, in_features, hidden_size, out_features, dropout, output=False):\n    super().__init__()\n    self.fc1 = nn.Linear(in_features, hidden_size)\n    self.fc2 = nn.Linear(hidden_size, out_features)\n    self.dropout = nn.Dropout(dropout)\n    self.output = output\n\n  def forward(self, x):\n    x = self.fc1(x)\n    x = self.dropout(x)\n    x = nn.functional.gelu(x)\n    x = self.fc2(x)\n    if not self.output:\n      x = self.dropout(x)\n    return x\n\nclass TransformerEncoder(nn.Module):\n  def __init__(self, n_patches, d_model, n_heads, mlp_size, dropout):\n    super().__init__()\n    self.ln1 = nn.LayerNorm([n_patches + 1, d_model])\n    self.ln2 = nn.LayerNorm([n_patches + 1, d_model])\n    self.msa = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n    self.mlp = MLP(d_model, mlp_size, d_model, dropout)\n\n  def forward(self, x):\n    x_skip = x\n    x = self.ln1(x)\n    x = self.msa(x, x, x)[0]\n    x = x + x_skip\n    x_skip = x\n    x = self.ln2(x)\n    x = self.mlp(x)\n    return x + x_skip\n\nclass TransformerEncoderLayer(nn.Module):\n\n  def __init__(self, n_patches, d_model, n_heads, mlp_size, layer, dropout):\n    super().__init__()\n    self.transformer_layers = nn.Sequential(*[TransformerEncoder(n_patches, d_model, n_heads, mlp_size, dropout) for i in range(layer)])\n\n  def forward(self, x):\n    return self.transformer_layers(x)\n\n\nclass VisionTransformer(nn.Module):\n  def __init__(self, layer, d_model, n_heads, mlp_size, patch_size, channels, n_patches, n_classes, dropout=0):\n    super().__init__()\n\n    self.linear_projection = nn.Parameter(torch.zeros((patch_size**2 * channels, d_model)))\n    self.class_token = nn.Parameter(torch.zeros(1, 1, d_model))\n    self.pos_embedding = nn.Parameter(torch.zeros(n_patches+1, d_model))\n    self.transformer_layers = TransformerEncoderLayer(n_patches, d_model, n_heads, mlp_size, layer, dropout)\n    self.mlp_head = MLP(d_model, mlp_size, n_classes, dropout, output=True)\n    self.dropout = nn.Dropout(dropout)\n\n\n    nn.init.normal_(self.linear_projection)\n    nn.init.normal_(self.class_token)\n    nn.init.normal_(self.pos_embedding)\n\n  def forward(self, x):\n    x = torch.flatten(x, 2, -1)\n    z0 = torch.einsum(\"nkp,pd -> nkd\", x, self.linear_projection)\n    class_token = self.class_token.expand(x.shape[0], -1, -1)\n    z0 = torch.cat([class_token, z0], dim=1) + self.pos_embedding\n    z0 = self.dropout(z0)\n    z0 = self.transformer_layers(z0)\n    y = z0[:, 0]\n    out = self.mlp_head(y)\n    return out","metadata":{"id":"adlCYSa1Xxei","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:02:18.207086Z","iopub.execute_input":"2025-05-18T16:02:18.207650Z","iopub.status.idle":"2025-05-18T16:02:18.218621Z","shell.execute_reply.started":"2025-05-18T16:02:18.207627Z","shell.execute_reply":"2025-05-18T16:02:18.217960Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"#transformer = VisionTransformer(layer=12, d_model=768, n_heads=12, mlp_size=3072, patch_size=12, channels=3, n_patches=9, n_classes=10)","metadata":{"id":"Zq8dyFvFu9Lk"},"outputs":[],"execution_count":13},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(patches, y_train, test_size=0.02, random_state=seed)","metadata":{"id":"EtN0tNPqnNWW","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:02:31.721974Z","iopub.execute_input":"2025-05-18T16:02:31.722589Z","iopub.status.idle":"2025-05-18T16:02:32.204071Z","shell.execute_reply.started":"2025-05-18T16:02:31.722565Z","shell.execute_reply":"2025-05-18T16:02:32.203118Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def test_model(lr):\n  torch.random.manual_seed(seed)\n  torch.cuda.manual_seed_all(seed)\n  torch.manual_seed(seed)\n  np.random.seed(seed)\n  random.seed(seed)\n  model = VisionTransformer(layer=12, d_model=768, n_heads=12, mlp_size=3072, patch_size=12, channels=3, n_patches=9, n_classes=10).to(device)\n  train_batches = DataLoader([*zip(X_train, y_train)], batch_size=512, shuffle=True)\n  val_batches = DataLoader([*zip(X_val, y_val)], batch_size=512, shuffle=True)\n  loss_fn = nn.CrossEntropyLoss()\n  val_loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 1000)\n  iterations = 0\n  over=False\n  while True:\n    for batch in train_batches:\n      model.train()\n      iterations += 1\n      optimizer.zero_grad()\n      features, target = batch[:-1], batch[-1]\n      features = features[0].to(device)\n      target = target.to(device)\n      outputs = model(features)\n      perte = loss_fn(outputs, target)\n      perte.backward()\n      torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n      optimizer.step()\n      scheduler.step()\n      if iterations % 100 == 0:\n        model.eval()\n        total_loss = 0\n        for batch in val_batches:\n          features, target = batch[:-1], batch[-1]\n          features = features[0].to(device)\n          target = target.to(device)\n          outputs = model(features)\n          perte = val_loss_fn(outputs, target)\n          total_loss += perte.item()\n        print(f\"Iteration {iterations}: Loss {total_loss/X_val.shape[0]}\")\n      if iterations == 20000:\n        del model, features, target\n        return total_loss\n","metadata":{"id":"ELsrqC3kEWoE","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:02:33.734520Z","iopub.execute_input":"2025-05-18T16:02:33.734773Z","iopub.status.idle":"2025-05-18T16:02:33.742558Z","shell.execute_reply.started":"2025-05-18T16:02:33.734756Z","shell.execute_reply":"2025-05-18T16:02:33.741801Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"min_error = 10**6\nmin_lr = None\n\nfor lr in [0.001, 0.003, 0.01, 0.03]:\n  print(\"Currently testing lr:\", lr)\n  err = test_model(lr)\n  if err < min_error:\n    min_error = err\n    min_lr = lr\n\nprint(f\"The learning rate chosen is: {min_lr}\")","metadata":{"id":"C3GM7IsbESJO","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:58:27.083359Z","iopub.execute_input":"2025-05-14T18:58:27.083702Z","execution_failed":"2025-05-15T06:50:55.399Z"}},"outputs":[{"name":"stdout","text":"Currently testing lr: 0.001\nIteration 100: Loss 2.17852294921875\nIteration 200: Loss 2.0801536865234374\nIteration 300: Loss 2.0406788330078127\nIteration 400: Loss 2.0003892211914063\nIteration 500: Loss 1.9865011596679687\nIteration 600: Loss 1.9752722778320313\nIteration 700: Loss 1.9727693481445312\nIteration 800: Loss 1.9691539916992187\nIteration 900: Loss 1.967044189453125\nIteration 1000: Loss 1.9669603881835938\nIteration 1100: Loss 1.9667037963867187\nIteration 1200: Loss 1.9653912963867188\nIteration 1300: Loss 1.96433984375\nIteration 1400: Loss 1.9592341918945313\nIteration 1500: Loss 1.956561767578125\nIteration 1600: Loss 1.9523311157226562\nIteration 1700: Loss 1.9519368896484375\nIteration 1800: Loss 1.944051025390625\nIteration 1900: Loss 1.9343546752929688\nIteration 2000: Loss 1.9359947509765625\nIteration 2100: Loss 1.9245740966796876\nIteration 2200: Loss 1.9311153564453125\nIteration 2300: Loss 1.9175234985351564\nIteration 2400: Loss 1.9175988159179687\nIteration 2500: Loss 1.9123463134765626\nIteration 2600: Loss 1.915541259765625\nIteration 2700: Loss 1.9087066040039062\nIteration 2800: Loss 1.9121444091796875\nIteration 2900: Loss 1.90906396484375\nIteration 3000: Loss 1.9092064208984374\nIteration 3100: Loss 1.909164794921875\nIteration 3200: Loss 1.9092718505859374\nIteration 3300: Loss 1.9084442138671875\nIteration 3400: Loss 1.9096761474609374\nIteration 3500: Loss 1.9094085693359375\nIteration 3600: Loss 1.9103193359375\nIteration 3700: Loss 1.9051170654296874\nIteration 3800: Loss 1.9047008056640624\nIteration 3900: Loss 1.9002699584960938\nIteration 4000: Loss 1.8975728759765624\nIteration 4100: Loss 1.8940247802734376\nIteration 4200: Loss 1.8987285766601563\nIteration 4300: Loss 1.8911992797851562\nIteration 4400: Loss 1.895310546875\nIteration 4500: Loss 1.8894205932617187\nIteration 4600: Loss 1.8866116943359375\nIteration 4700: Loss 1.8842890625\nIteration 4800: Loss 1.883796875\nIteration 4900: Loss 1.8838017578125\nIteration 5000: Loss 1.8839534301757812\nIteration 5100: Loss 1.8837509765625\nIteration 5200: Loss 1.88233251953125\nIteration 5300: Loss 1.8820816650390626\nIteration 5400: Loss 1.88122314453125\nIteration 5500: Loss 1.882710693359375\nIteration 5600: Loss 1.8839299926757813\nIteration 5700: Loss 1.8788727416992188\nIteration 5800: Loss 1.875970458984375\nIteration 5900: Loss 1.8781981201171876\nIteration 6000: Loss 1.8712745361328125\nIteration 6100: Loss 1.873986083984375\nIteration 6200: Loss 1.872179931640625\nIteration 6300: Loss 1.863262451171875\nIteration 6400: Loss 1.8719041748046874\nIteration 6500: Loss 1.8653683471679687\nIteration 6600: Loss 1.8621455078125\nIteration 6700: Loss 1.8632753295898437\nIteration 6800: Loss 1.8615833740234375\nIteration 6900: Loss 1.8595382080078124\nIteration 7000: Loss 1.860357421875\nIteration 7100: Loss 1.8601629638671875\nIteration 7200: Loss 1.85992724609375\nIteration 7300: Loss 1.8613887329101562\nIteration 7400: Loss 1.8599489135742187\nIteration 7500: Loss 1.8596104736328125\nIteration 7600: Loss 1.8558092651367188\nIteration 7700: Loss 1.8545717163085937\nIteration 7800: Loss 1.8570725708007811\nIteration 7900: Loss 1.8522836303710937\nIteration 8000: Loss 1.8510449829101563\nIteration 8100: Loss 1.8501111450195313\nIteration 8200: Loss 1.8455519409179688\nIteration 8300: Loss 1.8481668701171876\nIteration 8400: Loss 1.84391845703125\nIteration 8500: Loss 1.8439968872070311\nIteration 8600: Loss 1.8411236572265626\nIteration 8700: Loss 1.8382216796875\nIteration 8800: Loss 1.8359703369140625\nIteration 8900: Loss 1.8369385986328124\nIteration 9000: Loss 1.837001220703125\nIteration 9100: Loss 1.8373632202148438\nIteration 9200: Loss 1.8383919677734375\nIteration 9300: Loss 1.8383622436523437\nIteration 9400: Loss 1.840969482421875\nIteration 9500: Loss 1.8369970703125\nIteration 9600: Loss 1.8442839965820312\nIteration 9700: Loss 1.8351674194335938\nIteration 9800: Loss 1.8289760131835937\nIteration 9900: Loss 1.8427527465820313\nIteration 10000: Loss 1.829337890625\nIteration 10100: Loss 1.82597216796875\nIteration 10200: Loss 1.8230687866210937\nIteration 10300: Loss 1.835378662109375\nIteration 10400: Loss 1.8196525268554689\nIteration 10500: Loss 1.8202891845703124\nIteration 10600: Loss 1.81904296875\nIteration 10700: Loss 1.8136209716796876\nIteration 10800: Loss 1.8136151733398438\nIteration 10900: Loss 1.8155238647460938\nIteration 11000: Loss 1.814351806640625\nIteration 11100: Loss 1.8140113525390624\nIteration 11200: Loss 1.8135454711914063\nIteration 11300: Loss 1.8166693115234376\nIteration 11400: Loss 1.8140054931640626\nIteration 11500: Loss 1.8190264282226563\nIteration 11600: Loss 1.8178591918945313\nIteration 11700: Loss 1.8092657470703124\nIteration 11800: Loss 1.815642578125\nIteration 11900: Loss 1.8061316528320313\nIteration 12000: Loss 1.8083540649414063\nIteration 12100: Loss 1.8106534423828125\nIteration 12200: Loss 1.8046215209960939\nIteration 12300: Loss 1.8019053344726563\nIteration 12400: Loss 1.799135009765625\nIteration 12500: Loss 1.7979165649414062\nIteration 12600: Loss 1.7947086181640626\nIteration 12700: Loss 1.7911085205078126\nIteration 12800: Loss 1.7934210205078125\nIteration 12900: Loss 1.790928466796875\nIteration 13000: Loss 1.7909329833984375\nIteration 13100: Loss 1.7905977783203124\nIteration 13200: Loss 1.7933167724609376\nIteration 13300: Loss 1.7911737060546875\nIteration 13400: Loss 1.7903131713867189\nIteration 13500: Loss 1.7875282592773438\nIteration 13600: Loss 1.7890311279296875\nIteration 13700: Loss 1.7954887084960938\nIteration 13800: Loss 1.7866492919921875\nIteration 13900: Loss 1.7824520874023437\nIteration 14000: Loss 1.780013916015625\nIteration 14100: Loss 1.7827112426757812\nIteration 14200: Loss 1.7845900268554689\nIteration 14300: Loss 1.7839017333984375\nIteration 14400: Loss 1.7725658569335938\nIteration 14500: Loss 1.7776851196289063\nIteration 14600: Loss 1.7669808349609375\nIteration 14700: Loss 1.7712303466796875\nIteration 14800: Loss 1.7703602294921874\nIteration 14900: Loss 1.76733642578125\nIteration 15000: Loss 1.7678070068359375\nIteration 15100: Loss 1.7676630249023437\nIteration 15200: Loss 1.7663590698242186\nIteration 15300: Loss 1.76883056640625\nIteration 15400: Loss 1.7687659301757812\nIteration 15500: Loss 1.7640907592773438\nIteration 15600: Loss 1.7639277954101562\nIteration 15700: Loss 1.7638768920898438\nIteration 15800: Loss 1.7628798217773438\nIteration 15900: Loss 1.7659476318359375\nIteration 16000: Loss 1.7694155883789062\nIteration 16100: Loss 1.758392578125\nIteration 16200: Loss 1.764401611328125\nIteration 16300: Loss 1.7526903076171876\nIteration 16400: Loss 1.7472299194335938\nIteration 16500: Loss 1.7556859741210937\nIteration 16600: Loss 1.74750830078125\nIteration 16700: Loss 1.7426881103515626\nIteration 16800: Loss 1.7460040893554687\nIteration 16900: Loss 1.7454405517578124\nIteration 17000: Loss 1.7441450805664063\nIteration 17100: Loss 1.7450278930664063\nIteration 17200: Loss 1.7447100830078126\nIteration 17300: Loss 1.7466424560546876\nIteration 17400: Loss 1.7494755859375\nIteration 17500: Loss 1.7509708862304687\nIteration 17600: Loss 1.7412694091796874\nIteration 17700: Loss 1.74525439453125\nIteration 17800: Loss 1.7456282348632812\nIteration 17900: Loss 1.745962890625\nIteration 18000: Loss 1.74857177734375\nIteration 18100: Loss 1.7359810791015624\nIteration 18200: Loss 1.7269818725585937\nIteration 18300: Loss 1.736235595703125\nIteration 18400: Loss 1.7343726196289062\nIteration 18500: Loss 1.7356889038085936\nIteration 18600: Loss 1.725440185546875\nIteration 18700: Loss 1.72310107421875\nIteration 18800: Loss 1.7237672119140626\nIteration 18900: Loss 1.7221033325195312\nIteration 19000: Loss 1.7233707885742187\nIteration 19100: Loss 1.7233383178710937\nIteration 19200: Loss 1.7216065673828125\nIteration 19300: Loss 1.7231820068359376\nIteration 19400: Loss 1.7210897827148437\nIteration 19500: Loss 1.72348193359375\nIteration 19600: Loss 1.7288281860351562\nIteration 19700: Loss 1.7219556884765626\nIteration 19800: Loss 1.72842333984375\nIteration 19900: Loss 1.7124395751953125\nIteration 20000: Loss 1.7133458251953124\nCurrently testing lr: 0.003\nIteration 100: Loss 2.0508385009765626\nIteration 200: Loss 1.9865816040039062\nIteration 300: Loss 1.95143994140625\nIteration 400: Loss 1.9369783935546876\nIteration 500: Loss 1.9298786010742188\nIteration 600: Loss 1.9229773559570313\nIteration 700: Loss 1.92491650390625\nIteration 800: Loss 1.920154541015625\nIteration 900: Loss 1.913584716796875\nIteration 1000: Loss 1.9151262817382813\nIteration 1100: Loss 1.9156522216796874\nIteration 1200: Loss 1.9153228759765626\nIteration 1300: Loss 1.9118017578125\nIteration 1400: Loss 1.911958740234375\nIteration 1500: Loss 1.913034423828125\nIteration 1600: Loss 1.9106922607421875\nIteration 1700: Loss 1.90532470703125\nIteration 1800: Loss 1.9020404052734374\nIteration 1900: Loss 1.8886889038085937\nIteration 2000: Loss 1.8835694580078124\nIteration 2100: Loss 1.8879892578125\nIteration 2200: Loss 1.8951610107421875\nIteration 2300: Loss 1.8715957641601562\nIteration 2400: Loss 1.871090087890625\nIteration 2500: Loss 1.8541525268554688\nIteration 2600: Loss 1.8561402587890625\nIteration 2700: Loss 1.856478515625\nIteration 2800: Loss 1.8487282104492186\nIteration 2900: Loss 1.847305419921875\nIteration 3000: Loss 1.8487811889648438\nIteration 3100: Loss 1.8496551513671875\nIteration 3200: Loss 1.845908447265625\nIteration 3300: Loss 1.8494935913085937\nIteration 3400: Loss 1.8503927001953124\nIteration 3500: Loss 1.8426599731445312\nIteration 3600: Loss 1.846330810546875\nIteration 3700: Loss 1.8460390014648438\nIteration 3800: Loss 1.8381430053710937\nIteration 3900: Loss 1.8269859619140625\nIteration 4000: Loss 1.827761474609375\nIteration 4100: Loss 1.8400928344726561\nIteration 4200: Loss 1.8169121704101563\nIteration 4300: Loss 1.8201223754882812\nIteration 4400: Loss 1.8116614990234374\nIteration 4500: Loss 1.79567333984375\nIteration 4600: Loss 1.7991571044921875\nIteration 4700: Loss 1.7878305053710937\nIteration 4800: Loss 1.7860199584960939\nIteration 4900: Loss 1.7842301635742188\nIteration 5000: Loss 1.7854413452148437\nIteration 5100: Loss 1.785197998046875\nIteration 5200: Loss 1.7856072387695312\nIteration 5300: Loss 1.7894154663085937\nIteration 5400: Loss 1.7859838256835938\nIteration 5500: Loss 1.7836084594726562\nIteration 5600: Loss 1.7941028442382811\nIteration 5700: Loss 1.77157861328125\nIteration 5800: Loss 1.773561279296875\nIteration 5900: Loss 1.7872669067382811\nIteration 6000: Loss 1.759405517578125\nIteration 6100: Loss 1.7646029663085938\nIteration 6200: Loss 1.7511468505859376\nIteration 6300: Loss 1.74272412109375\nIteration 6400: Loss 1.7458992919921874\nIteration 6500: Loss 1.740385009765625\nIteration 6600: Loss 1.749483642578125\nIteration 6700: Loss 1.7395209350585938\nIteration 6800: Loss 1.7238562622070313\nIteration 6900: Loss 1.7239212036132812\nIteration 7000: Loss 1.7237393188476562\nIteration 7100: Loss 1.72337939453125\nIteration 7200: Loss 1.7222168579101562\nIteration 7300: Loss 1.7282427978515624\nIteration 7400: Loss 1.719933837890625\nIteration 7500: Loss 1.726242919921875\nIteration 7600: Loss 1.7215489501953125\nIteration 7700: Loss 1.7063977661132812\nIteration 7800: Loss 1.7176700439453125\nIteration 7900: Loss 1.721175537109375\nIteration 8000: Loss 1.705404541015625\nIteration 8100: Loss 1.6964983520507813\nIteration 8200: Loss 1.692233642578125\nIteration 8300: Loss 1.6866192016601562\nIteration 8400: Loss 1.6856135864257813\nIteration 8500: Loss 1.6799728393554687\nIteration 8600: Loss 1.6767259521484374\nIteration 8700: Loss 1.6705341796875\nIteration 8800: Loss 1.6614723510742186\nIteration 8900: Loss 1.6606323852539062\nIteration 9000: Loss 1.6599556274414062\nIteration 9100: Loss 1.6623078002929688\nIteration 9200: Loss 1.6595526123046875\nIteration 9300: Loss 1.6557222900390625\nIteration 9400: Loss 1.6649749755859375\nIteration 9500: Loss 1.6615120239257812\nIteration 9600: Loss 1.655369384765625\nIteration 9700: Loss 1.6471354370117188\nIteration 9800: Loss 1.6504788208007812\nIteration 9900: Loss 1.6365397338867187\nIteration 10000: Loss 1.6312769775390625\nIteration 10100: Loss 1.6232705688476563\nIteration 10200: Loss 1.6276738891601563\nIteration 10300: Loss 1.6338993530273438\nIteration 10400: Loss 1.61268359375\nIteration 10500: Loss 1.605894775390625\nIteration 10600: Loss 1.609875732421875\nIteration 10700: Loss 1.6010451049804688\nIteration 10800: Loss 1.5984182739257813\nIteration 10900: Loss 1.5963836669921876\nIteration 11000: Loss 1.5956375732421875\nIteration 11100: Loss 1.595503173828125\nIteration 11200: Loss 1.594263427734375\nIteration 11300: Loss 1.5940491333007814\nIteration 11400: Loss 1.5928726806640625\nIteration 11500: Loss 1.6126477661132812\nIteration 11600: Loss 1.6000906982421874\nIteration 11700: Loss 1.592010986328125\nIteration 11800: Loss 1.5896627807617187\nIteration 11900: Loss 1.5719161376953126\nIteration 12000: Loss 1.5894879150390624\nIteration 12100: Loss 1.585667724609375\nIteration 12200: Loss 1.58229296875\nIteration 12300: Loss 1.5685044555664063\nIteration 12400: Loss 1.5443447265625\nIteration 12500: Loss 1.5608538208007812\nIteration 12600: Loss 1.5492999267578125\nIteration 12700: Loss 1.5441373291015625\nIteration 12800: Loss 1.54100732421875\nIteration 12900: Loss 1.539953369140625\nIteration 13000: Loss 1.5391533813476563\nIteration 13100: Loss 1.5387802734375\nIteration 13200: Loss 1.5369780883789061\nIteration 13300: Loss 1.5485186767578125\nIteration 13400: Loss 1.5420845947265625\nIteration 13500: Loss 1.5389639892578124\nIteration 13600: Loss 1.5362913818359376\nIteration 13700: Loss 1.538974365234375\nIteration 13800: Loss 1.5355823974609375\nIteration 13900: Loss 1.5263973999023437\nIteration 14000: Loss 1.52328466796875\nIteration 14100: Loss 1.5197224731445313\nIteration 14200: Loss 1.5305322875976564\nIteration 14300: Loss 1.51729833984375\nIteration 14400: Loss 1.5198845825195313\nIteration 14500: Loss 1.5055072631835937\nIteration 14600: Loss 1.5072057495117188\nIteration 14700: Loss 1.5023883056640626\nIteration 14800: Loss 1.492063720703125\nIteration 14900: Loss 1.4945687255859375\nIteration 15000: Loss 1.4927608032226563\nIteration 15100: Loss 1.4919158325195312\nIteration 15200: Loss 1.4948555908203125\nIteration 15300: Loss 1.4984680786132814\nIteration 15400: Loss 1.4936659545898439\nIteration 15500: Loss 1.4910704956054688\nIteration 15600: Loss 1.4973491821289062\nIteration 15700: Loss 1.4963242797851561\nIteration 15800: Loss 1.4935480346679688\nIteration 15900: Loss 1.5029526977539063\nIteration 16000: Loss 1.4759052124023437\nIteration 16100: Loss 1.508990966796875\nIteration 16200: Loss 1.4771658325195312\nIteration 16300: Loss 1.486312744140625\nIteration 16400: Loss 1.465530517578125\nIteration 16500: Loss 1.4603515014648438\nIteration 16600: Loss 1.4609962158203125\nIteration 16700: Loss 1.4565051879882813\nIteration 16800: Loss 1.4512025756835938\nIteration 16900: Loss 1.4491260986328125\nIteration 17000: Loss 1.44933837890625\nIteration 17100: Loss 1.450044921875\nIteration 17200: Loss 1.449727294921875\nIteration 17300: Loss 1.4475224609375\nIteration 17400: Loss 1.4560175170898437\nIteration 17500: Loss 1.4505525512695312\nIteration 17600: Loss 1.4527828369140625\nIteration 17700: Loss 1.4385391845703126\nIteration 17800: Loss 1.4591359252929688\nIteration 17900: Loss 1.4409564819335938\nIteration 18000: Loss 1.4343303833007812\nIteration 18100: Loss 1.4349590454101562\nIteration 18200: Loss 1.4300870361328124\nIteration 18300: Loss 1.435338623046875\nIteration 18400: Loss 1.4342761840820313\nIteration 18500: Loss 1.4169199829101562\nIteration 18600: Loss 1.415609130859375\nIteration 18700: Loss 1.4113392944335938\nIteration 18800: Loss 1.4114026489257812\nIteration 18900: Loss 1.4076712646484375\nIteration 19000: Loss 1.407978271484375\nIteration 19100: Loss 1.4076665649414062\nIteration 19200: Loss 1.4071390380859374\nIteration 19300: Loss 1.4128329467773437\nIteration 19400: Loss 1.41013037109375\nIteration 19500: Loss 1.413983154296875\nIteration 19600: Loss 1.4166472778320311\nIteration 19700: Loss 1.4089595947265625\nIteration 19800: Loss 1.4098851318359376\nIteration 19900: Loss 1.40885595703125\nIteration 20000: Loss 1.4034491577148438\nCurrently testing lr: 0.01\nIteration 100: Loss 1.982242431640625\nIteration 200: Loss 1.9592686157226562\nIteration 300: Loss 1.9076957397460939\nIteration 400: Loss 1.892860595703125\nIteration 500: Loss 1.8807515258789063\nIteration 600: Loss 1.8674205932617187\nIteration 700: Loss 1.8692074584960938\nIteration 800: Loss 1.8518797607421875\nIteration 900: Loss 1.851481201171875\nIteration 1000: Loss 1.8524962158203124\nIteration 1100: Loss 1.8515333251953126\nIteration 1200: Loss 1.8477855224609374\nIteration 1300: Loss 1.8623739624023437\nIteration 1400: Loss 1.846939208984375\nIteration 1500: Loss 1.850939697265625\nIteration 1600: Loss 1.8290136108398438\nIteration 1700: Loss 1.8207162475585938\nIteration 1800: Loss 1.792864013671875\nIteration 1900: Loss 1.8232410278320312\nIteration 2000: Loss 1.7804913940429687\nIteration 2100: Loss 1.7964934692382812\nIteration 2200: Loss 1.7578609619140626\nIteration 2300: Loss 1.7287144165039063\nIteration 2400: Loss 1.7257636108398438\nIteration 2500: Loss 1.6923607177734374\nIteration 2600: Loss 1.6703113403320313\nIteration 2700: Loss 1.6708251953125\nIteration 2800: Loss 1.658822265625\nIteration 2900: Loss 1.6587870483398437\nIteration 3000: Loss 1.6567366943359374\nIteration 3100: Loss 1.6601414794921876\nIteration 3200: Loss 1.6560845336914063\nIteration 3300: Loss 1.6505413818359376\nIteration 3400: Loss 1.6464090576171875\nIteration 3500: Loss 1.651037841796875\nIteration 3600: Loss 1.641482177734375\nIteration 3700: Loss 1.6225961303710938\nIteration 3800: Loss 1.627358154296875\nIteration 3900: Loss 1.60256494140625\nIteration 4000: Loss 1.5794189453125\nIteration 4100: Loss 1.5578639526367188\nIteration 4200: Loss 1.5603623657226562\nIteration 4300: Loss 1.5450409545898438\nIteration 4400: Loss 1.526085205078125\nIteration 4500: Loss 1.5371174926757813\nIteration 4600: Loss 1.5067125854492187\nIteration 4700: Loss 1.48975244140625\nIteration 4800: Loss 1.4870859375\nIteration 4900: Loss 1.47391552734375\nIteration 5000: Loss 1.4748402099609375\nIteration 5100: Loss 1.4742417602539062\nIteration 5200: Loss 1.4720592651367188\nIteration 5300: Loss 1.4792385864257813\nIteration 5400: Loss 1.48307861328125\nIteration 5500: Loss 1.4802937622070313\nIteration 5600: Loss 1.4682498168945313\nIteration 5700: Loss 1.4547734985351561\nIteration 5800: Loss 1.4389688110351562\nIteration 5900: Loss 1.4603037719726562\nIteration 6000: Loss 1.430144287109375\nIteration 6100: Loss 1.4289368286132813\nIteration 6200: Loss 1.4144488525390626\nIteration 6300: Loss 1.4188509521484376\nIteration 6400: Loss 1.3750850830078125\nIteration 6500: Loss 1.371920166015625\nIteration 6600: Loss 1.3780224609375\nIteration 6700: Loss 1.3601683349609375\nIteration 6800: Loss 1.3417924194335937\nIteration 6900: Loss 1.344995361328125\nIteration 7000: Loss 1.3411055908203124\nIteration 7100: Loss 1.3414262084960937\nIteration 7200: Loss 1.3397151489257813\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\nlr = 0.01\n\nprint(\"Currently testing lr:\", lr)\nerr = test_model(lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T19:11:17.352888Z","iopub.execute_input":"2025-05-15T19:11:17.353147Z","iopub.status.idle":"2025-05-16T00:45:40.336631Z","shell.execute_reply.started":"2025-05-15T19:11:17.353126Z","shell.execute_reply":"2025-05-16T00:45:40.335879Z"}},"outputs":[{"name":"stdout","text":"Currently testing lr: 0.01\nIteration 100: Loss 1.982242431640625\nIteration 200: Loss 1.9592686157226562\nIteration 300: Loss 1.9076957397460939\nIteration 400: Loss 1.892860595703125\nIteration 500: Loss 1.8807515258789063\nIteration 600: Loss 1.8674205932617187\nIteration 700: Loss 1.8692074584960938\nIteration 800: Loss 1.8518797607421875\nIteration 900: Loss 1.851481201171875\nIteration 1000: Loss 1.8524962158203124\nIteration 1100: Loss 1.8515333251953126\nIteration 1200: Loss 1.8477855224609374\nIteration 1300: Loss 1.8623739624023437\nIteration 1400: Loss 1.846939208984375\nIteration 1500: Loss 1.850939697265625\nIteration 1600: Loss 1.8290136108398438\nIteration 1700: Loss 1.8207162475585938\nIteration 1800: Loss 1.792864013671875\nIteration 1900: Loss 1.8232410278320312\nIteration 2000: Loss 1.7804913940429687\nIteration 2100: Loss 1.7964934692382812\nIteration 2200: Loss 1.7578609619140626\nIteration 2300: Loss 1.7287144165039063\nIteration 2400: Loss 1.7257636108398438\nIteration 2500: Loss 1.6923607177734374\nIteration 2600: Loss 1.6703113403320313\nIteration 2700: Loss 1.6708251953125\nIteration 2800: Loss 1.658822265625\nIteration 2900: Loss 1.6587870483398437\nIteration 3000: Loss 1.6567366943359374\nIteration 3100: Loss 1.6601414794921876\nIteration 3200: Loss 1.6560845336914063\nIteration 3300: Loss 1.6505413818359376\nIteration 3400: Loss 1.6464090576171875\nIteration 3500: Loss 1.651037841796875\nIteration 3600: Loss 1.641482177734375\nIteration 3700: Loss 1.6225961303710938\nIteration 3800: Loss 1.627358154296875\nIteration 3900: Loss 1.60256494140625\nIteration 4000: Loss 1.5794189453125\nIteration 4100: Loss 1.5578639526367188\nIteration 4200: Loss 1.5603623657226562\nIteration 4300: Loss 1.5450409545898438\nIteration 4400: Loss 1.526085205078125\nIteration 4500: Loss 1.5371174926757813\nIteration 4600: Loss 1.5067125854492187\nIteration 4700: Loss 1.48975244140625\nIteration 4800: Loss 1.4870859375\nIteration 4900: Loss 1.47391552734375\nIteration 5000: Loss 1.4748402099609375\nIteration 5100: Loss 1.4742417602539062\nIteration 5200: Loss 1.4720592651367188\nIteration 5300: Loss 1.4792385864257813\nIteration 5400: Loss 1.48307861328125\nIteration 5500: Loss 1.4802937622070313\nIteration 5600: Loss 1.4682498168945313\nIteration 5700: Loss 1.4547734985351561\nIteration 5800: Loss 1.4389688110351562\nIteration 5900: Loss 1.4603037719726562\nIteration 6000: Loss 1.430144287109375\nIteration 6100: Loss 1.4289368286132813\nIteration 6200: Loss 1.4144488525390626\nIteration 6300: Loss 1.4188509521484376\nIteration 6400: Loss 1.3750850830078125\nIteration 6500: Loss 1.371920166015625\nIteration 6600: Loss 1.3780224609375\nIteration 6700: Loss 1.3601683349609375\nIteration 6800: Loss 1.3417924194335937\nIteration 6900: Loss 1.344995361328125\nIteration 7000: Loss 1.3411055908203124\nIteration 7100: Loss 1.3414262084960937\nIteration 7200: Loss 1.3397151489257813\nIteration 7300: Loss 1.34557763671875\nIteration 7400: Loss 1.357564208984375\nIteration 7500: Loss 1.3451414184570312\nIteration 7600: Loss 1.3414393310546875\nIteration 7700: Loss 1.3408413696289063\nIteration 7800: Loss 1.3432273559570314\nIteration 7900: Loss 1.3200473022460937\nIteration 8000: Loss 1.3316697387695313\nIteration 8100: Loss 1.3178573608398438\nIteration 8200: Loss 1.3156511840820313\nIteration 8300: Loss 1.3158699951171875\nIteration 8400: Loss 1.3096664428710938\nIteration 8500: Loss 1.297367431640625\nIteration 8600: Loss 1.3089615478515626\nIteration 8700: Loss 1.2799663696289063\nIteration 8800: Loss 1.2772321166992187\nIteration 8900: Loss 1.26935693359375\nIteration 9000: Loss 1.2690843505859375\nIteration 9100: Loss 1.2724088134765625\nIteration 9200: Loss 1.271569580078125\nIteration 9300: Loss 1.2667626342773437\nIteration 9400: Loss 1.280196533203125\nIteration 9500: Loss 1.2795591430664062\nIteration 9600: Loss 1.2765634155273438\nIteration 9700: Loss 1.2883526611328124\nIteration 9800: Loss 1.3075891723632813\nIteration 9900: Loss 1.312\nIteration 10000: Loss 1.3223703002929688\nIteration 10100: Loss 1.3016824951171875\nIteration 10200: Loss 1.262395263671875\nIteration 10300: Loss 1.2954190673828125\nIteration 10400: Loss 1.2899000854492189\nIteration 10500: Loss 1.2564251708984375\nIteration 10600: Loss 1.271673828125\nIteration 10700: Loss 1.2551107177734375\nIteration 10800: Loss 1.2502684326171876\nIteration 10900: Loss 1.243203125\nIteration 11000: Loss 1.2427371826171876\nIteration 11100: Loss 1.2452659912109374\nIteration 11200: Loss 1.2463562622070312\nIteration 11300: Loss 1.2542716674804688\nIteration 11400: Loss 1.2718025512695312\nIteration 11500: Loss 1.271105712890625\nIteration 11600: Loss 1.2946203002929688\nIteration 11700: Loss 1.263340576171875\nIteration 11800: Loss 1.2694072875976563\nIteration 11900: Loss 1.3056229248046876\nIteration 12000: Loss 1.2615924682617188\nIteration 12100: Loss 1.2966097412109374\nIteration 12200: Loss 1.2841605834960939\nIteration 12300: Loss 1.24662548828125\nIteration 12400: Loss 1.27078125\nIteration 12500: Loss 1.2682205200195313\nIteration 12600: Loss 1.263971435546875\nIteration 12700: Loss 1.2726510009765626\nIteration 12800: Loss 1.2643905029296876\nIteration 12900: Loss 1.2588088989257813\nIteration 13000: Loss 1.2563062744140625\nIteration 13100: Loss 1.2592138061523437\nIteration 13200: Loss 1.259819580078125\nIteration 13300: Loss 1.28346435546875\nIteration 13400: Loss 1.27911083984375\nIteration 13500: Loss 1.2788724365234374\nIteration 13600: Loss 1.2902373657226562\nIteration 13700: Loss 1.310982666015625\nIteration 13800: Loss 1.3195904541015624\nIteration 13900: Loss 1.3007853393554687\nIteration 14000: Loss 1.3129026489257813\nIteration 14100: Loss 1.304969970703125\nIteration 14200: Loss 1.3707369995117187\nIteration 14300: Loss 1.3205752563476563\nIteration 14400: Loss 1.3532457885742188\nIteration 14500: Loss 1.334517333984375\nIteration 14600: Loss 1.3306257934570314\nIteration 14700: Loss 1.3380232543945312\nIteration 14800: Loss 1.3332005615234375\nIteration 14900: Loss 1.3272947998046876\nIteration 15000: Loss 1.3264158325195312\nIteration 15100: Loss 1.330583251953125\nIteration 15200: Loss 1.3363654174804687\nIteration 15300: Loss 1.336973876953125\nIteration 15400: Loss 1.345713623046875\nIteration 15500: Loss 1.36631982421875\nIteration 15600: Loss 1.3936552734375\nIteration 15700: Loss 1.3850005493164061\nIteration 15800: Loss 1.4226063232421875\nIteration 15900: Loss 1.4288511962890624\nIteration 16000: Loss 1.452398681640625\nIteration 16100: Loss 1.403673583984375\nIteration 16200: Loss 1.4459204711914062\nIteration 16300: Loss 1.4469276733398437\nIteration 16400: Loss 1.4319285278320313\nIteration 16500: Loss 1.4724671020507814\nIteration 16600: Loss 1.4762064208984376\nIteration 16700: Loss 1.4653233642578125\nIteration 16800: Loss 1.45880419921875\nIteration 16900: Loss 1.460002685546875\nIteration 17000: Loss 1.46057958984375\nIteration 17100: Loss 1.4607135620117186\nIteration 17200: Loss 1.4663776245117188\nIteration 17300: Loss 1.48046533203125\nIteration 17400: Loss 1.4936051635742187\nIteration 17500: Loss 1.5103994140625\nIteration 17600: Loss 1.5384046020507813\nIteration 17700: Loss 1.5242109375\nIteration 17800: Loss 1.5853943481445312\nIteration 17900: Loss 1.5960697021484376\nIteration 18000: Loss 1.628630126953125\nIteration 18100: Loss 1.608056640625\nIteration 18200: Loss 1.6248358154296876\nIteration 18300: Loss 1.6449509887695313\nIteration 18400: Loss 1.6609024658203124\nIteration 18500: Loss 1.6472449340820312\nIteration 18600: Loss 1.6451112060546875\nIteration 18700: Loss 1.674224853515625\nIteration 18800: Loss 1.6684990844726562\nIteration 18900: Loss 1.6648404541015625\nIteration 19000: Loss 1.6629773559570313\nIteration 19100: Loss 1.66590283203125\nIteration 19200: Loss 1.6735803833007812\nIteration 19300: Loss 1.6839482421875\nIteration 19400: Loss 1.7386790161132812\nIteration 19500: Loss 1.7045997314453125\nIteration 19600: Loss 1.790599853515625\nIteration 19700: Loss 1.7704058227539063\nIteration 19800: Loss 1.76050048828125\nIteration 19900: Loss 1.7987139282226563\nIteration 20000: Loss 1.8482498168945312\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"\nlr = 0.03\n\nprint(\"Currently testing lr:\", lr)\nerr = test_model(lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T00:45:40.339539Z","iopub.execute_input":"2025-05-16T00:45:40.339760Z","iopub.status.idle":"2025-05-16T02:01:36.989213Z","shell.execute_reply.started":"2025-05-16T00:45:40.339743Z","shell.execute_reply":"2025-05-16T02:01:36.988163Z"}},"outputs":[{"name":"stdout","text":"Currently testing lr: 0.03\nIteration 100: Loss 1.9483055419921875\nIteration 200: Loss 1.9030689697265626\nIteration 300: Loss 1.8389835815429687\nIteration 400: Loss 1.7852335815429687\nIteration 500: Loss 1.7535896606445311\nIteration 600: Loss 1.7310767822265625\nIteration 700: Loss 1.6822145385742187\nIteration 800: Loss 1.6629968872070313\nIteration 900: Loss 1.652461669921875\nIteration 1000: Loss 1.65121875\nIteration 1100: Loss 1.6529755859375\nIteration 1200: Loss 1.6461273803710939\nIteration 1300: Loss 1.6409962768554687\nIteration 1400: Loss 1.6303756103515625\nIteration 1500: Loss 1.6188065185546876\nIteration 1600: Loss 1.6315648803710938\nIteration 1700: Loss 1.5675325927734376\nIteration 1800: Loss 1.562041015625\nIteration 1900: Loss 1.5080671997070312\nIteration 2000: Loss 1.4927706298828125\nIteration 2100: Loss 1.4659063110351562\nIteration 2200: Loss 1.4345357055664063\nIteration 2300: Loss 1.375076904296875\nIteration 2400: Loss 1.3707401123046874\nIteration 2500: Loss 1.3455325317382814\nIteration 2600: Loss 1.28935791015625\nIteration 2700: Loss 1.2982789306640625\nIteration 2800: Loss 1.2813310546875\nIteration 2900: Loss 1.2720502319335938\nIteration 3000: Loss 1.2719647827148437\nIteration 3100: Loss 1.2754756469726563\nIteration 3200: Loss 1.2794110717773437\nIteration 3300: Loss 1.2757847900390624\nIteration 3400: Loss 1.2702528686523438\nIteration 3500: Loss 1.2971900024414063\nIteration 3600: Loss 1.2892188110351563\nIteration 3700: Loss 1.3285132446289063\nIteration 3800: Loss 1.3211014404296875\nIteration 3900: Loss 1.3503303833007811\nIteration 4000: Loss 1.33259326171875\nIteration 4100: Loss 1.3467965698242188\nIteration 4200: Loss 1.3067791137695313\nIteration 4300: Loss 1.3682384033203125\nIteration 4400: Loss 1.364345458984375\nIteration 4500: Loss 1.432732666015625\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_44/3093058287.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Currently testing lr:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_44/3784431951.py\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(lr)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"X_train = torch.concat([X_train, X_val])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:03:34.511109Z","iopub.execute_input":"2025-05-18T16:03:34.511909Z","iopub.status.idle":"2025-05-18T16:03:34.932983Z","shell.execute_reply.started":"2025-05-18T16:03:34.511880Z","shell.execute_reply":"2025-05-18T16:03:34.932421Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"y_train = torch.concat([y_train, y_val])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:03:56.680631Z","iopub.execute_input":"2025-05-18T16:03:56.680891Z","iopub.status.idle":"2025-05-18T16:03:56.685146Z","shell.execute_reply.started":"2025-05-18T16:03:56.680873Z","shell.execute_reply":"2025-05-18T16:03:56.684357Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"X_train.shape\ny_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:04:01.002803Z","iopub.execute_input":"2025-05-18T16:04:01.003359Z","iopub.status.idle":"2025-05-18T16:04:01.008902Z","shell.execute_reply.started":"2025-05-18T16:04:01.003330Z","shell.execute_reply":"2025-05-18T16:04:01.008122Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"torch.Size([50000, 10])"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"lr = 0.03  \ntorch.random.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\nmodel = VisionTransformer(layer=12, d_model=768, n_heads=12, mlp_size=3072, patch_size=12, channels=3, n_patches=9, n_classes=10).to(device)\ntrain_batches = DataLoader([*zip(X_train, y_train)], batch_size=512, shuffle=True)\nval_batches = DataLoader([*zip(X_val, y_val)], batch_size=512, shuffle=True)\nloss_fn = nn.CrossEntropyLoss()\nval_loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 1000)\niterations = 0\nover=False\nwhile True:\n    for batch in train_batches:\n        model.train()\n        iterations += 1\n        optimizer.zero_grad()\n        features, target = batch[:-1], batch[-1]\n        features = features[0].to(device)\n        target = target.to(device)\n        outputs = model(features)\n        perte = loss_fn(outputs, target)\n        perte.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n        optimizer.step()\n        scheduler.step()\n        if iterations % 100 == 0:\n            print(f\"Iteration {iterations}: DONE\")\n        if iterations == 3400:\n            over=True\n            break\n    if over:\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:07:23.088610Z","iopub.execute_input":"2025-05-18T16:07:23.089106Z","iopub.status.idle":"2025-05-18T16:58:34.420217Z","shell.execute_reply.started":"2025-05-18T16:07:23.089085Z","shell.execute_reply":"2025-05-18T16:58:34.419423Z"}},"outputs":[{"name":"stdout","text":"Iteration 100: DONE\nIteration 200: DONE\nIteration 300: DONE\nIteration 400: DONE\nIteration 500: DONE\nIteration 600: DONE\nIteration 700: DONE\nIteration 800: DONE\nIteration 900: DONE\nIteration 1000: DONE\nIteration 1100: DONE\nIteration 1200: DONE\nIteration 1300: DONE\nIteration 1400: DONE\nIteration 1500: DONE\nIteration 1600: DONE\nIteration 1700: DONE\nIteration 1800: DONE\nIteration 1900: DONE\nIteration 2000: DONE\nIteration 2100: DONE\nIteration 2200: DONE\nIteration 2300: DONE\nIteration 2400: DONE\nIteration 2500: DONE\nIteration 2600: DONE\nIteration 2700: DONE\nIteration 2800: DONE\nIteration 2900: DONE\nIteration 3000: DONE\nIteration 3100: DONE\nIteration 3200: DONE\nIteration 3300: DONE\nIteration 3400: DONE\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"channel_1 = test_images[:, 0, :, :]\nchannel_2 = test_images[:, 1, :, :]\nchannel_3 = test_images[:, 2, :, :]\nchannel_1_patches = []\nchannel_2_patches = []\nchannel_3_patches = []\nfor i in range(channel_1.shape[0]):\n  channel_1_patches.append(blockshaped(channel_1[i], 12, 12))\nchannel_1_patches = torch.stack(channel_1_patches)\nfor i in range(channel_2.shape[0]):\n  channel_2_patches.append(blockshaped(channel_2[i], 12, 12))\nchannel_2_patches = torch.stack(channel_2_patches)\nfor i in range(channel_3.shape[0]):\n  channel_3_patches.append(blockshaped(channel_3[i], 12, 12))\nchannel_3_patches = torch.stack(channel_3_patches)\npatches = torch.stack([channel_1_patches, channel_2_patches, channel_3_patches], dim=1).permute(0, 2, 1, 3, 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:12:06.337769Z","iopub.execute_input":"2025-05-18T17:12:06.338471Z","iopub.status.idle":"2025-05-18T17:12:07.081893Z","shell.execute_reply.started":"2025-05-18T17:12:06.338447Z","shell.execute_reply":"2025-05-18T17:12:07.081078Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"patches.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:12:11.214787Z","iopub.execute_input":"2025-05-18T17:12:11.215116Z","iopub.status.idle":"2025-05-18T17:12:11.220812Z","shell.execute_reply.started":"2025-05-18T17:12:11.215092Z","shell.execute_reply":"2025-05-18T17:12:11.220006Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"torch.Size([10000, 9, 3, 12, 12])"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"model.eval()\ntest_loader = DataLoader([*zip(patches, test_labels)], batch_size=512, shuffle=False)\ncorrect = 0\nfor batch in test_loader:\n  features, target = batch[:-1], batch[-1]\n  features = features[0].to(device)\n  target = target.to(device)\n  outputs = model(features)\n  correct += torch.where(torch.argmax(torch.softmax(outputs, dim=1), dim=1)==target, 1, 0).sum()\nprint((correct/test_images.shape[0])*100)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-648IiXU-yt3","outputId":"af9ff984-fce4-47d6-a6a0-768c675ccb16","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:12:24.494407Z","iopub.execute_input":"2025-05-18T17:12:24.495064Z","iopub.status.idle":"2025-05-18T17:12:30.964777Z","shell.execute_reply.started":"2025-05-18T17:12:24.495033Z","shell.execute_reply":"2025-05-18T17:12:30.963931Z"}},"outputs":[{"name":"stdout","text":"tensor(55.2700, device='cuda:0')\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"del model, features, target","metadata":{"id":"8Sm14JEmAxi-"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Have to pretrain or augment data to get better results","metadata":{}}]}