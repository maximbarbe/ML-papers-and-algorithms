{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision\nfrom torchvision.transforms import v2\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom random import choices\nimport numpy as np\nimport random\nfrom sklearn.model_selection import train_test_split\nimport warnings","metadata":{"id":"LX1zgjQrJJeI","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:22:08.547487Z","iopub.execute_input":"2025-05-13T03:22:08.547788Z","iopub.status.idle":"2025-05-13T03:22:16.150911Z","shell.execute_reply.started":"2025-05-13T03:22:08.547764Z","shell.execute_reply":"2025-05-13T03:22:16.150187Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nseed = 42\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"AzBP5GV1JM5f","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:22:16.152075Z","iopub.execute_input":"2025-05-13T03:22:16.152391Z","iopub.status.idle":"2025-05-13T03:22:16.239101Z","shell.execute_reply.started":"2025-05-13T03:22:16.152374Z","shell.execute_reply":"2025-05-13T03:22:16.238359Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"torch.random.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)","metadata":{"id":"wd41xS2oJON0","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:22:16.240007Z","iopub.execute_input":"2025-05-13T03:22:16.240335Z","iopub.status.idle":"2025-05-13T03:22:16.263783Z","shell.execute_reply.started":"2025-05-13T03:22:16.240300Z","shell.execute_reply":"2025-05-13T03:22:16.262997Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"training_dataset = torchvision.datasets.CIFAR10(root=\"./\", download=True, train=True)\ntest_dataset = torchvision.datasets.CIFAR10(root=\"./\", download=True, train=False)","metadata":{"id":"PYWG4MXbJQuB","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:22:16.265402Z","iopub.execute_input":"2025-05-13T03:22:16.265668Z","iopub.status.idle":"2025-05-13T03:22:24.654561Z","shell.execute_reply.started":"2025-05-13T03:22:16.265647Z","shell.execute_reply":"2025-05-13T03:22:24.653999Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 170M/170M [00:04<00:00, 35.0MB/s] \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"training_images = []\ntest_images = []\ntransformation = torchvision.transforms.ToTensor()\n\nfor img in training_dataset.data:\n  training_images.append(transformation(img))\n\nfor img in test_dataset.data:\n  test_images.append(transformation(img))\n\ntraining_images = torch.stack(training_images)\ntest_images = torch.stack(test_images)","metadata":{"id":"FoxsL91GJZEJ","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:22:24.655344Z","iopub.execute_input":"2025-05-13T03:22:24.655584Z","iopub.status.idle":"2025-05-13T03:22:26.991626Z","shell.execute_reply.started":"2025-05-13T03:22:24.655557Z","shell.execute_reply":"2025-05-13T03:22:26.990807Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"per_pixel_mean = torch.sum(training_images, dim=0)/training_images.shape[0]\ntraining_images = training_images - per_pixel_mean\ntest_images = test_images - per_pixel_mean","metadata":{"id":"rOdKEd6ZLE40","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:22:26.993686Z","iopub.execute_input":"2025-05-13T03:22:26.994284Z","iopub.status.idle":"2025-05-13T03:22:27.478900Z","shell.execute_reply.started":"2025-05-13T03:22:26.994252Z","shell.execute_reply":"2025-05-13T03:22:27.478124Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_labels = training_dataset.targets\ntest_labels = test_dataset.targets\ny_train = torch.Tensor([[1 if i == el else 0 for i in range(10)] for el in train_labels])\ny_test = torch.Tensor([[1 if i == el else 0 for i in range(10)] for el in test_labels])","metadata":{"id":"Oi2MDQXomzef","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:22:27.479714Z","iopub.execute_input":"2025-05-13T03:22:27.479959Z","iopub.status.idle":"2025-05-13T03:22:27.735911Z","shell.execute_reply.started":"2025-05-13T03:22:27.479934Z","shell.execute_reply":"2025-05-13T03:22:27.735393Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\nclass ResidualBlock(nn.Module):\n\n  def __init__(self, in_channels, out_channels, kernel, padding1, padding2, stride):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.upsampling = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride)\n\n    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel, padding=padding1, stride=stride)\n    self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel, padding=padding2, stride=1)\n    self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n    self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n\n    nn.init.normal_(self.upsampling.weight, 0, (2/20)**0.5)\n    nn.init.normal_(self.conv1.weight, 0, (2/20)**0.5)\n    nn.init.normal_(self.conv2.weight, 0, (2/20)**0.5)\n\n    nn.init.zeros_(self.upsampling.bias)\n    nn.init.zeros_(self.conv1.bias)\n    nn.init.zeros_(self.conv2.bias)\n\n  def forward(self, x):\n    if self.in_channels != self.out_channels:\n      x_skip = self.upsampling(x)\n    else:\n      x_skip = x\n\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = nn.functional.relu(x)\n    x = self.conv2(x)\n    x = x + x_skip\n    x = self.bn2(x)\n    x = nn.functional.relu(x)\n    return x\n\n\nclass ResNet20(nn.Module):\n\n  def __init__(self):\n    super().__init__()\n    self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n\n    self.res_block_16_1 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_2 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_3 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_4 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_5 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_6 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_32_1 = ResidualBlock(in_channels=16, out_channels=32, kernel=3, padding1=1, padding2=1, stride=2)\n    self.res_block_32_2 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_3 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_4 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_5 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_6 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_1 = ResidualBlock(in_channels=32, out_channels=64, kernel=3, padding1=1, padding2=1, stride=2)\n    self.res_block_64_2 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_3 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_4 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_5 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_6 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n\n    self.global_pooling = nn.AvgPool2d(kernel_size=8)\n    self.fc = nn.Linear(64, 10)\n\n    nn.init.normal_(self.conv1.weight, 0, (2/20)**0.5)\n    nn.init.normal_(self.fc.weight, 0, (2/20)**0.5)\n\n    nn.init.zeros_(self.conv1.bias)\n    nn.init.zeros_(self.fc.bias)\n\n\n  def forward(self, x):\n    x = self.conv1(x)\n    x = self.res_block_16_1(x)\n    x = self.res_block_16_2(x)\n    x = self.res_block_16_3(x)\n    x = self.res_block_16_4(x)\n    x = self.res_block_16_5(x)\n    x = self.res_block_16_6(x)\n    x = self.res_block_32_1(x)\n    x = self.res_block_32_2(x)\n    x = self.res_block_32_3(x)\n    x = self.res_block_32_4(x)\n    x = self.res_block_32_5(x)\n    x = self.res_block_32_6(x)\n    x = self.res_block_64_1(x)\n    x = self.res_block_64_2(x)\n    x = self.res_block_64_3(x)\n    x = self.res_block_64_4(x)\n    x = self.res_block_64_5(x)\n    x = self.res_block_64_6(x)\n    x = self.global_pooling(x)\n    x = torch.flatten(x, 1, -1)\n    x = self.fc(x)\n    return x","metadata":{"id":"adlCYSa1Xxei"},"outputs":[],"execution_count":72},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(training_images, y_train, test_size=0.1, random_state=seed)","metadata":{"id":"EtN0tNPqnNWW","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:22:27.736615Z","iopub.execute_input":"2025-05-13T03:22:27.736878Z","iopub.status.idle":"2025-05-13T03:22:28.346304Z","shell.execute_reply.started":"2025-05-13T03:22:27.736853Z","shell.execute_reply":"2025-05-13T03:22:28.345716Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"transforms = v2.Compose([\n    v2.RandomHorizontalFlip(p=0.5),\n    v2.RandomCrop(size=32, padding=4)\n])\n\ntemp_x = []\ntemp_y = []\nfor img, label in [*zip(X_train, y_train)]:\n  for i in range(5):\n    temp_x.append(transforms(img))\n    temp_y.append(label)\nX_train = torch.stack(temp_x)\ny_train = torch.stack(temp_y)","metadata":{"id":"XUDQd7ruVDmr","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:22:28.347082Z","iopub.execute_input":"2025-05-13T03:22:28.347321Z","iopub.status.idle":"2025-05-13T03:23:02.285431Z","shell.execute_reply.started":"2025-05-13T03:22:28.347305Z","shell.execute_reply":"2025-05-13T03:23:02.284637Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(X_train.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CczReU6QXY4i","outputId":"cb379e7b-8b3d-4069-cf9f-8a50fdc9a792","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:23:02.286312Z","iopub.execute_input":"2025-05-13T03:23:02.286578Z","iopub.status.idle":"2025-05-13T03:23:02.290844Z","shell.execute_reply.started":"2025-05-13T03:23:02.286555Z","shell.execute_reply":"2025-05-13T03:23:02.290307Z"}},"outputs":[{"name":"stdout","text":"torch.Size([225000, 3, 32, 32])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"weight_decay = 0.0001\nmomentum = 0.9\nlr = 0.1\nbatch_size = 128\niterations = 0\nover=False\n\ntrain_batches = DataLoader([*zip(X_train, y_train)], batch_size=batch_size, shuffle=True)\nval_batches = DataLoader([*zip(X_val, y_val)], batch_size=batch_size, shuffle=True)\nloss_fn = nn.CrossEntropyLoss()\nval_loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\nmodel = ResNet20().to(device)\noptimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n\nwhile True:\n\n  for batch in train_batches:\n    model.train()\n    iterations += 1\n    optimizer.zero_grad()\n    features, target = batch[:-1], batch[-1]\n    features = features[0].to(device)\n    target = target.to(device)\n    outputs = model(features)\n    perte = loss_fn(outputs, target)\n    perte.backward()\n    optimizer.step()\n\n    if iterations % 100 == 0:\n      model.eval()\n      total_loss = 0\n      for batch in val_batches:\n        features, target = batch[:-1], batch[-1]\n        features = features[0].to(device)\n        target = target.to(device)\n        outputs = model(features)\n        perte = val_loss_fn(outputs, target)\n        total_loss += perte.item()\n      print(f\"Iteration {iterations}: Loss {total_loss/X_val.shape[0]}\")\n\n    match iterations:\n      case 32000:\n        lr /= 10\n        optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n      case 48000:\n        lr /= 10\n        optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n      case 64000:\n        over=True\n        break\n      case _:\n        continue\n\n\n\n\n  if over:\n    break\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HqP4ILV_lsJb","outputId":"87ebc039-3d15-490d-e18d-c0bfbc414e55"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 100: Loss 2.1981354187011717\n","Iteration 200: Loss 2.0027960945129393\n","Iteration 300: Loss 2.002994049835205\n","Iteration 400: Loss 1.9953965141296386\n","Iteration 500: Loss 1.858583112335205\n","Iteration 600: Loss 1.8534700119018555\n","Iteration 700: Loss 1.7887409023284913\n","Iteration 800: Loss 1.7238394760131837\n","Iteration 900: Loss 1.8912242183685302\n","Iteration 1000: Loss 1.6848548057556152\n","Iteration 1100: Loss 1.8760100011825562\n","Iteration 1200: Loss 1.6785180240631103\n","Iteration 1300: Loss 1.6412097129821777\n","Iteration 1400: Loss 1.6558067039489746\n","Iteration 1500: Loss 1.5729736562728882\n","Iteration 1600: Loss 1.6453614837646484\n","Iteration 1700: Loss 1.5380710227966308\n","Iteration 1800: Loss 1.4981574514389038\n","Iteration 1900: Loss 1.5487183439254761\n","Iteration 2000: Loss 1.5583930297851563\n","Iteration 2100: Loss 1.4706424869537353\n","Iteration 2200: Loss 1.504853060913086\n","Iteration 2300: Loss 1.3554442335128785\n","Iteration 2400: Loss 1.4096236949920655\n","Iteration 2500: Loss 1.5129338760375977\n","Iteration 2600: Loss 1.3788561723709107\n","Iteration 2700: Loss 1.4160978668212891\n","Iteration 2800: Loss 1.358813066291809\n","Iteration 2900: Loss 1.4077165008544923\n","Iteration 3000: Loss 1.2920162782669067\n","Iteration 3100: Loss 1.374477894592285\n","Iteration 3200: Loss 1.3088713750839234\n","Iteration 3300: Loss 1.4061029096603395\n","Iteration 3400: Loss 1.2387240768432617\n","Iteration 3500: Loss 1.2902790573120118\n","Iteration 3600: Loss 1.2027398834228515\n","Iteration 3700: Loss 1.292624164390564\n","Iteration 3800: Loss 1.1450143238067627\n","Iteration 3900: Loss 1.1070077693939209\n","Iteration 4000: Loss 1.2949966737747192\n","Iteration 4100: Loss 1.2692862228393555\n","Iteration 4200: Loss 1.156119276714325\n","Iteration 4300: Loss 1.2274979133605957\n","Iteration 4400: Loss 0.9948038297653198\n","Iteration 4500: Loss 1.123677512741089\n","Iteration 4600: Loss 1.1573337982177734\n","Iteration 4700: Loss 1.155767398071289\n","Iteration 4800: Loss 1.0653623758792876\n","Iteration 4900: Loss 0.9833880475044251\n","Iteration 5000: Loss 1.1560209107398987\n","Iteration 5100: Loss 1.1824668563842773\n","Iteration 5200: Loss 1.2337391448974608\n","Iteration 5300: Loss 0.9776244250297547\n","Iteration 5400: Loss 1.0056265171051026\n","Iteration 5500: Loss 1.252029609298706\n","Iteration 5600: Loss 0.8720754137039185\n","Iteration 5700: Loss 1.0136084367752076\n","Iteration 5800: Loss 0.9600608390808105\n","Iteration 5900: Loss 0.9811441934585571\n","Iteration 6000: Loss 0.969308971786499\n","Iteration 6100: Loss 0.8620147444725037\n","Iteration 6200: Loss 1.0048693846702577\n","Iteration 6300: Loss 0.9758739990234375\n","Iteration 6400: Loss 0.828128035068512\n","Iteration 6500: Loss 0.8850569231033325\n","Iteration 6600: Loss 0.9065817623138428\n","Iteration 6700: Loss 0.8718712186813354\n","Iteration 6800: Loss 0.8560724061965942\n","Iteration 6900: Loss 0.8723441463470459\n","Iteration 7000: Loss 1.119236120080948\n","Iteration 7100: Loss 0.8394100806236268\n","Iteration 7200: Loss 0.9027213148117066\n","Iteration 7300: Loss 1.0633574920654296\n","Iteration 7400: Loss 0.96201462392807\n","Iteration 7500: Loss 0.9661665357589722\n","Iteration 7600: Loss 0.8241789799690247\n","Iteration 7700: Loss 0.7414906078338623\n","Iteration 7800: Loss 0.7267607206344604\n","Iteration 7900: Loss 0.8464061826705933\n","Iteration 8000: Loss 1.0613181106567382\n","Iteration 8100: Loss 0.8155628596305847\n","Iteration 8200: Loss 0.8314202586889267\n","Iteration 8300: Loss 0.7850993912220001\n","Iteration 8400: Loss 0.7843285011768341\n","Iteration 8500: Loss 0.7596543471813202\n","Iteration 8600: Loss 0.8359665147781372\n","Iteration 8700: Loss 0.7535865061759949\n","Iteration 8800: Loss 0.6753253616333008\n","Iteration 8900: Loss 0.9391842672586441\n","Iteration 9000: Loss 0.7229604716300965\n","Iteration 9100: Loss 0.9078552893638611\n","Iteration 9200: Loss 0.8299152053833008\n","Iteration 9300: Loss 1.0288027338027954\n","Iteration 9400: Loss 0.7851610818862915\n","Iteration 9500: Loss 0.855747399520874\n","Iteration 9600: Loss 0.7641374069213868\n","Iteration 9700: Loss 0.7842670186042786\n","Iteration 9800: Loss 0.6498686849594116\n","Iteration 9900: Loss 0.6655351203918457\n","Iteration 10000: Loss 0.7427025390625\n","Iteration 10100: Loss 0.7057774176597595\n","Iteration 10200: Loss 0.6826939536094665\n","Iteration 10300: Loss 0.6602455976963043\n","Iteration 10400: Loss 0.681709956073761\n","Iteration 10500: Loss 0.6466840414047241\n","Iteration 10600: Loss 0.6935101140975952\n","Iteration 10700: Loss 0.6956518390655517\n","Iteration 10800: Loss 0.7486562298297882\n","Iteration 10900: Loss 0.8264714015007019\n","Iteration 11000: Loss 0.8864435263633728\n","Iteration 11100: Loss 0.626230198097229\n","Iteration 11200: Loss 0.8646700105667114\n","Iteration 11300: Loss 0.7442679535388946\n","Iteration 11400: Loss 0.8536160791397095\n","Iteration 11500: Loss 0.6217603500366211\n","Iteration 11600: Loss 0.6095732660770417\n","Iteration 11700: Loss 0.6150097561836243\n","Iteration 11800: Loss 0.8499471328735352\n","Iteration 11900: Loss 0.7075588600516319\n","Iteration 12000: Loss 0.5985289564132691\n","Iteration 12100: Loss 0.6251592723846435\n","Iteration 12200: Loss 0.7555457911491394\n","Iteration 12300: Loss 0.7223004503250122\n","Iteration 12400: Loss 0.8163877012252808\n","Iteration 12500: Loss 0.9177774624824524\n","Iteration 12600: Loss 0.5639577271938324\n","Iteration 12700: Loss 0.7797494805335998\n","Iteration 12800: Loss 0.6467246480941773\n","Iteration 12900: Loss 0.584769183921814\n","Iteration 13000: Loss 0.5891445932388306\n","Iteration 13100: Loss 0.6242980606079102\n","Iteration 13200: Loss 0.6573545314788818\n","Iteration 13300: Loss 0.7083917690038681\n","Iteration 13400: Loss 0.6675056323051453\n","Iteration 13500: Loss 0.6426481983184814\n","Iteration 13600: Loss 0.6938305720806122\n","Iteration 13700: Loss 0.6370032341003418\n","Iteration 13800: Loss 0.7599954205513001\n","Iteration 13900: Loss 0.6619274448394775\n","Iteration 14000: Loss 0.6227597517967224\n","Iteration 14100: Loss 0.6832854582309723\n","Iteration 14200: Loss 0.7365962084770202\n","Iteration 14300: Loss 0.9035302815437317\n","Iteration 14400: Loss 0.6064351114273071\n","Iteration 14500: Loss 0.7204618782043457\n","Iteration 14600: Loss 0.6372099149703979\n","Iteration 14700: Loss 0.72378590965271\n","Iteration 14800: Loss 0.5761474186897277\n","Iteration 14900: Loss 0.6775692119598389\n","Iteration 15000: Loss 0.6167386247158051\n","Iteration 15100: Loss 0.8364769538879394\n","Iteration 15200: Loss 0.5925611772537231\n","Iteration 15300: Loss 0.6218445217132569\n","Iteration 15400: Loss 0.7145326167583466\n","Iteration 15500: Loss 0.5969913246154785\n","Iteration 15600: Loss 0.6856318159103394\n","Iteration 15700: Loss 0.6013384508132935\n","Iteration 15800: Loss 0.6792747344970703\n","Iteration 15900: Loss 0.7410020304679871\n","Iteration 16000: Loss 0.6601527772426605\n","Iteration 16100: Loss 0.5990337028503417\n","Iteration 16200: Loss 0.5642511994838715\n","Iteration 16300: Loss 0.533752457857132\n","Iteration 16400: Loss 0.5859827660560608\n","Iteration 16500: Loss 0.7697847554206848\n","Iteration 16600: Loss 0.5886137401580811\n","Iteration 16700: Loss 0.5689447224617005\n","Iteration 16800: Loss 0.5882559503555298\n","Iteration 16900: Loss 0.562314396572113\n","Iteration 17000: Loss 0.744082081413269\n","Iteration 17100: Loss 0.6577704044103623\n","Iteration 17200: Loss 0.6322814220428467\n","Iteration 17300: Loss 0.6085535642623902\n","Iteration 17400: Loss 0.5862703890889883\n","Iteration 17500: Loss 0.7263569482803345\n","Iteration 17600: Loss 0.5683738842010498\n","Iteration 17700: Loss 0.6113293556213379\n","Iteration 17800: Loss 0.571348782157898\n","Iteration 17900: Loss 0.7619612097740174\n","Iteration 18000: Loss 0.6843358879089355\n","Iteration 18100: Loss 0.6580869949579239\n","Iteration 18200: Loss 0.5553136602401734\n","Iteration 18300: Loss 0.5363608884334564\n","Iteration 18400: Loss 0.5896715347290039\n","Iteration 18500: Loss 0.6201553731918334\n","Iteration 18600: Loss 0.49981999073028566\n","Iteration 18700: Loss 0.6069649116516114\n","Iteration 18800: Loss 0.602975862455368\n","Iteration 18900: Loss 0.8229151136398315\n","Iteration 19000: Loss 0.5311757596492768\n","Iteration 19100: Loss 0.5174578668594361\n","Iteration 19200: Loss 0.7632721315383911\n","Iteration 19300: Loss 0.6378181469917298\n","Iteration 19400: Loss 0.5442200149536133\n","Iteration 19500: Loss 0.5946688029289245\n","Iteration 19600: Loss 0.5535075174331665\n","Iteration 19700: Loss 0.5900364280700684\n","Iteration 19800: Loss 0.5208896096229553\n","Iteration 19900: Loss 0.5801171521186829\n","Iteration 20000: Loss 0.7467789050102234\n","Iteration 20100: Loss 0.5034981126308441\n","Iteration 20200: Loss 0.6137001649856567\n","Iteration 20300: Loss 0.5291244851112366\n","Iteration 20400: Loss 0.529835451990366\n","Iteration 20500: Loss 0.5975488869667053\n","Iteration 20600: Loss 0.4905015037894249\n","Iteration 20700: Loss 0.5420084433555603\n","Iteration 20800: Loss 0.5589116970062256\n","Iteration 20900: Loss 0.5182078709602356\n","Iteration 21000: Loss 0.508153309738636\n","Iteration 21100: Loss 0.5749728506088256\n","Iteration 21200: Loss 0.7744715169906616\n","Iteration 21300: Loss 0.6168624963760376\n","Iteration 21400: Loss 0.6124053112030029\n","Iteration 21500: Loss 0.606516681098938\n","Iteration 21600: Loss 0.7756185744285583\n","Iteration 21700: Loss 0.6049178453564644\n","Iteration 21800: Loss 0.5831205370426178\n","Iteration 21900: Loss 0.6175872367858887\n","Iteration 22000: Loss 0.6209434975624084\n","Iteration 22100: Loss 0.6614384360313416\n","Iteration 22200: Loss 0.6541471367835998\n","Iteration 22300: Loss 0.5397726142883301\n","Iteration 22400: Loss 0.5459118972301483\n","Iteration 22500: Loss 0.7269230445861816\n","Iteration 22600: Loss 0.4959580783843994\n","Iteration 22700: Loss 0.6320041660308838\n","Iteration 22800: Loss 0.5464078543186188\n","Iteration 22900: Loss 0.5579023134231568\n","Iteration 23000: Loss 0.5851924399375915\n","Iteration 23100: Loss 0.6543175363063812\n","Iteration 23200: Loss 0.5815195847511292\n","Iteration 23300: Loss 0.5484935944318772\n","Iteration 23400: Loss 0.6449195932865143\n","Iteration 23500: Loss 0.5862245904445649\n","Iteration 23600: Loss 0.6829144924163818\n","Iteration 23700: Loss 0.690769918525219\n","Iteration 23800: Loss 0.49906836290359496\n","Iteration 23900: Loss 0.5212026997566224\n","Iteration 24000: Loss 0.6295814216852188\n","Iteration 24100: Loss 0.5296336242675781\n","Iteration 24200: Loss 0.5153462808609008\n","Iteration 24300: Loss 0.5895673011779785\n","Iteration 24400: Loss 0.5026469830989838\n","Iteration 24500: Loss 0.6025217431545258\n","Iteration 24600: Loss 0.5700243891239166\n","Iteration 24700: Loss 0.5983580758571625\n","Iteration 24800: Loss 0.5972949975445867\n","Iteration 24900: Loss 0.6700700998306275\n","Iteration 25000: Loss 0.5043492874145508\n","Iteration 25100: Loss 0.5461000859379769\n","Iteration 25200: Loss 0.6457756220519543\n","Iteration 25300: Loss 0.6599031792640686\n","Iteration 25400: Loss 0.5733699231147766\n","Iteration 25500: Loss 0.5433971843719483\n","Iteration 25600: Loss 0.5650825506210327\n","Iteration 25700: Loss 0.7614721054553986\n","Iteration 25800: Loss 0.557113039302826\n","Iteration 25900: Loss 0.5356798813819885\n","Iteration 26000: Loss 0.48323547134399414\n","Iteration 26100: Loss 0.5728300924301147\n","Iteration 26200: Loss 0.5490969371795654\n","Iteration 26300: Loss 0.5175743206977844\n","Iteration 26400: Loss 0.6623246047973633\n","Iteration 26500: Loss 0.7420008117675782\n","Iteration 26600: Loss 0.7183735410690307\n","Iteration 26700: Loss 0.5213709776878357\n","Iteration 26800: Loss 0.5646253161430359\n","Iteration 26900: Loss 0.5241060610324144\n","Iteration 27000: Loss 0.6196565547943115\n","Iteration 27100: Loss 0.5747642127037048\n","Iteration 27200: Loss 0.6228981068611145\n","Iteration 27300: Loss 0.615765503025055\n","Iteration 27400: Loss 0.527010782623291\n","Iteration 27500: Loss 0.5760103356838226\n","Iteration 27600: Loss 0.6800533930182457\n","Iteration 27700: Loss 0.5802235034942627\n","Iteration 27800: Loss 0.567236462855339\n","Iteration 27900: Loss 0.44714680442810056\n","Iteration 28000: Loss 0.6087682759284974\n","Iteration 28100: Loss 0.5612157367706299\n","Iteration 28200: Loss 0.48668217527866364\n","Iteration 28300: Loss 0.5330261238098144\n","Iteration 28400: Loss 0.6787728420257568\n","Iteration 28500: Loss 1.0162859838485718\n","Iteration 28600: Loss 0.5208320080757141\n","Iteration 28700: Loss 0.5616005675315857\n","Iteration 28800: Loss 0.5746890072345734\n","Iteration 28900: Loss 0.5702529978752137\n","Iteration 29000: Loss 0.5951867530822754\n","Iteration 29100: Loss 0.5503755651473999\n","Iteration 29200: Loss 0.49909397621154783\n","Iteration 29300: Loss 0.6967090005397797\n","Iteration 29400: Loss 0.5436222323417663\n","Iteration 29500: Loss 0.5598139878273011\n","Iteration 29600: Loss 0.4679383275270462\n","Iteration 29700: Loss 0.5607765910625457\n","Iteration 29800: Loss 0.556916438472271\n","Iteration 29900: Loss 0.5333188302695752\n","Iteration 30000: Loss 0.5366882687568665\n","Iteration 30100: Loss 0.567583337020874\n","Iteration 30200: Loss 0.5087365109920502\n","Iteration 30300: Loss 0.7077316661834717\n","Iteration 30400: Loss 0.6415273852348328\n","Iteration 30500: Loss 0.5091303472518921\n","Iteration 30600: Loss 0.5794823247909546\n","Iteration 30700: Loss 0.6232429352760315\n","Iteration 30800: Loss 0.48332164888381957\n","Iteration 30900: Loss 0.5416317841529846\n","Iteration 31000: Loss 0.49651201300621034\n","Iteration 31100: Loss 0.6061848618507385\n","Iteration 31200: Loss 0.5124842466831208\n","Iteration 31300: Loss 0.5510691669464112\n","Iteration 31400: Loss 0.6763436475753785\n","Iteration 31500: Loss 0.5076257994174957\n","Iteration 31600: Loss 0.49986768185943364\n","Iteration 31700: Loss 0.5421799866199494\n","Iteration 31800: Loss 0.5233885799407959\n","Iteration 31900: Loss 0.4608275337219238\n","Iteration 32000: Loss 0.5109607006072998\n","Iteration 32100: Loss 0.355035714006424\n","Iteration 32200: Loss 0.34431058278083804\n","Iteration 32300: Loss 0.3388808981895447\n","Iteration 32400: Loss 0.3361467041015625\n","Iteration 32500: Loss 0.33483589029312133\n","Iteration 32600: Loss 0.3302808052062988\n","Iteration 32700: Loss 0.33331179336309436\n","Iteration 32800: Loss 0.3397624929428101\n","Iteration 32900: Loss 0.3476411808013916\n","Iteration 33000: Loss 0.33770611457824706\n","Iteration 33100: Loss 0.3391813418865204\n","Iteration 33200: Loss 0.3402174580357969\n","Iteration 33300: Loss 0.3305025024414063\n","Iteration 33400: Loss 0.3389567427456379\n","Iteration 33500: Loss 0.33725097036361695\n","Iteration 33600: Loss 0.34547241649627686\n","Iteration 33700: Loss 0.3466060749053955\n","Iteration 33800: Loss 0.35473781323432924\n","Iteration 33900: Loss 0.35537718987604605\n","Iteration 34000: Loss 0.3523737706184387\n","Iteration 34100: Loss 0.3581163866050541\n","Iteration 34200: Loss 0.3611533886790276\n","Iteration 34300: Loss 0.3738407601766754\n","Iteration 34400: Loss 0.35548369354065507\n","Iteration 34500: Loss 0.3585358171463013\n","Iteration 34600: Loss 0.3613355053514242\n","Iteration 34700: Loss 0.36473968794122336\n","Iteration 34800: Loss 0.3627825043890625\n","Iteration 34900: Loss 0.3744687656261027\n","Iteration 35000: Loss 0.3706162992954254\n","Iteration 35100: Loss 0.3736348422527313\n","Iteration 35200: Loss 0.37240525383446366\n","Iteration 35300: Loss 0.3752713948249817\n","Iteration 35400: Loss 0.37847977998405696\n","Iteration 35500: Loss 0.38050989818573\n","Iteration 35600: Loss 0.3849683490753174\n","Iteration 35700: Loss 0.3889062906453386\n","Iteration 35800: Loss 0.39895041372179985\n","Iteration 35900: Loss 0.3958565429151058\n","Iteration 36000: Loss 0.40320016593933106\n","Iteration 36100: Loss 0.40376108360290525\n","Iteration 36200: Loss 0.40235819585323335\n","Iteration 36300: Loss 0.40089218425750733\n","Iteration 36400: Loss 0.39862970470786097\n","Iteration 36500: Loss 0.4030091194152832\n","Iteration 36600: Loss 0.4013966323852539\n","Iteration 36700: Loss 0.40868928921222686\n","Iteration 36800: Loss 0.40488905802965164\n","Iteration 36900: Loss 0.4108726136311889\n","Iteration 37000: Loss 0.4109423085689545\n","Iteration 37100: Loss 0.4065804656982422\n","Iteration 37200: Loss 0.40816114540100096\n","Iteration 37300: Loss 0.4190461835861206\n","Iteration 37400: Loss 0.4235166620016098\n","Iteration 37500: Loss 0.4262372759580612\n","Iteration 37600: Loss 0.424574764251709\n","Iteration 37700: Loss 0.42465711317062377\n","Iteration 37800: Loss 0.4220067660808563\n","Iteration 37900: Loss 0.4209049030303955\n","Iteration 38000: Loss 0.4138404925268143\n","Iteration 38100: Loss 0.4195617085769773\n","Iteration 38200: Loss 0.41944285373687745\n","Iteration 38300: Loss 0.4358844787597656\n","Iteration 38400: Loss 0.43815080699920655\n","Iteration 38500: Loss 0.4338499500731006\n","Iteration 38600: Loss 0.44567849159240724\n","Iteration 38700: Loss 0.44177365019842985\n","Iteration 38800: Loss 0.4451510273218155\n","Iteration 38900: Loss 0.4475769076514058\n","Iteration 39000: Loss 0.4486271866798401\n","Iteration 39100: Loss 0.44921461000442503\n","Iteration 39200: Loss 0.45741354055404665\n","Iteration 39300: Loss 0.45978151178359983\n","Iteration 39400: Loss 0.4561282810211182\n","Iteration 39500: Loss 0.45912282809615135\n","Iteration 39600: Loss 0.45684073457717894\n","Iteration 39700: Loss 0.4579827428914607\n","Iteration 39800: Loss 0.4554415672302246\n","Iteration 39900: Loss 0.455816530418396\n","Iteration 40000: Loss 0.45880577805787326\n","Iteration 40100: Loss 0.4672316367298365\n","Iteration 40200: Loss 0.46240703435125763\n","Iteration 40300: Loss 0.47170403947830203\n","Iteration 40400: Loss 0.4778690971374512\n","Iteration 40500: Loss 0.4744158197734505\n","Iteration 40600: Loss 0.47718508964776996\n","Iteration 40700: Loss 0.48024199867248535\n","Iteration 40800: Loss 0.47845833457764236\n","Iteration 40900: Loss 0.48485768375992777\n","Iteration 41000: Loss 0.48790764899253847\n","Iteration 41100: Loss 0.48613305348828434\n","Iteration 41200: Loss 0.4809829420089722\n","Iteration 41300: Loss 0.4874598853111267\n","Iteration 41400: Loss 0.48841616541743277\n","Iteration 41500: Loss 0.49392064924240114\n","Iteration 41600: Loss 0.4873155756473541\n","Iteration 41700: Loss 0.5058330221176147\n","Iteration 41800: Loss 0.5096301845788955\n","Iteration 41900: Loss 0.50449364528656\n","Iteration 42000: Loss 0.5077421339035034\n","Iteration 42100: Loss 0.50788014575243\n","Iteration 42200: Loss 0.5048925140380859\n","Iteration 42300: Loss 0.5084735897012055\n","Iteration 42400: Loss 0.5088387939453125\n","Iteration 42500: Loss 0.5329919759273529\n","Iteration 42600: Loss 0.5231224251270294\n","Iteration 42700: Loss 0.5164033319473267\n","Iteration 42800: Loss 0.5249868435293435\n","Iteration 42900: Loss 0.5185145339932292\n","Iteration 43000: Loss 0.5152506143569946\n","Iteration 43100: Loss 0.5144621087789536\n","Iteration 43200: Loss 0.5157875943183899\n","Iteration 43300: Loss 0.5253245431900024\n","Iteration 43400: Loss 0.5315191057682037\n","Iteration 43500: Loss 0.5300183866500855\n","Iteration 43600: Loss 0.524851239669323\n","Iteration 43700: Loss 0.5272201705837156\n","Iteration 43800: Loss 0.525301731300354\n","Iteration 43900: Loss 0.5185576412677765\n","Iteration 44000: Loss 0.533514181298064\n","Iteration 44100: Loss 0.5254611704826355\n","Iteration 44200: Loss 0.5300689775466919\n","Iteration 44300: Loss 0.5330514446020126\n","Iteration 44400: Loss 0.5364699992913636\n","Iteration 44500: Loss 0.5493262405395508\n","Iteration 44600: Loss 0.5417917266974458\n","Iteration 44700: Loss 0.5406261148452759\n","Iteration 44800: Loss 0.5502561122894287\n","Iteration 44900: Loss 0.5419060901641846\n","Iteration 45000: Loss 0.5505286665439606\n","Iteration 45100: Loss 0.5252671680688858\n","Iteration 45200: Loss 0.5332324107267894\n","Iteration 45300: Loss 0.5369928023917601\n","Iteration 45400: Loss 0.5348708169579506\n","Iteration 45500: Loss 0.5428162831306458\n","Iteration 45600: Loss 0.5464104193687439\n","Iteration 45700: Loss 0.5336001691818237\n","Iteration 45800: Loss 0.5295017523288726\n","Iteration 45900: Loss 0.5376895727157592\n","Iteration 46000: Loss 0.5493967587605119\n","Iteration 46100: Loss 0.5396314426751109\n","Iteration 46200: Loss 0.5534744675636292\n","Iteration 46300: Loss 0.5360158518493175\n","Iteration 46400: Loss 0.5381852431774139\n","Iteration 46500: Loss 0.5390118279218674\n","Iteration 46600: Loss 0.5420654118003324\n","Iteration 46700: Loss 0.5541219051837921\n","Iteration 46800: Loss 0.5490776239544153\n","Iteration 46900: Loss 0.5505748109817505\n","Iteration 47000: Loss 0.5458720428735018\n","Iteration 47100: Loss 0.5629545808792115\n","Iteration 47200: Loss 0.5656179058074952\n","Iteration 47300: Loss 0.5480042151540517\n","Iteration 47400: Loss 0.5411763868331909\n","Iteration 47500: Loss 0.555212934878259\n","Iteration 47600: Loss 0.5559266683101654\n","Iteration 47700: Loss 0.5554281146526336\n","Iteration 47800: Loss 0.5619900009155273\n","Iteration 47900: Loss 0.5542675171852112\n","Iteration 48000: Loss 0.5640983635187149\n","Iteration 48100: Loss 0.5617568084299565\n","Iteration 48200: Loss 0.5653729953765869\n","Iteration 48300: Loss 0.5637486566543579\n","Iteration 48400: Loss 0.5575061891406774\n","Iteration 48500: Loss 0.5609679166158544\n","Iteration 48600: Loss 0.5553827750504017\n","Iteration 48700: Loss 0.5575472703933716\n","Iteration 48800: Loss 0.5524005621477962\n","Iteration 48900: Loss 0.5503530053853574\n","Iteration 49000: Loss 0.5536051393508911\n","Iteration 49100: Loss 0.5534155841318192\n","Iteration 49200: Loss 0.5609749300956726\n","Iteration 49300: Loss 0.5547897608280182\n","Iteration 49400: Loss 0.5533275489804684\n","Iteration 49500: Loss 0.5544767809666693\n","Iteration 49600: Loss 0.5524800806045532\n","Iteration 49700: Loss 0.5561931891351007\n","Iteration 49800: Loss 0.5561460669100284\n","Iteration 49900: Loss 0.5554008933886886\n","Iteration 50000: Loss 0.5549103883743286\n","Iteration 50100: Loss 0.5554552543967962\n","Iteration 50200: Loss 0.5529294442996383\n","Iteration 50300: Loss 0.5577459934234619\n","Iteration 50400: Loss 0.5527351950231939\n","Iteration 50500: Loss 0.5568590381622315\n","Iteration 50600: Loss 0.5537963850021362\n","Iteration 50700: Loss 0.5503145435333252\n","Iteration 50800: Loss 0.5559793363332748\n","Iteration 50900: Loss 0.5529464030264324\n","Iteration 51000: Loss 0.5546413066148758\n","Iteration 51100: Loss 0.5571319324493408\n","Iteration 51200: Loss 0.5564999133825302\n","Iteration 51300: Loss 0.554421587395668\n","Iteration 51400: Loss 0.5578067888975143\n","Iteration 51500: Loss 0.5562202221691609\n","Iteration 51600: Loss 0.5563294750213623\n","Iteration 51700: Loss 0.559281069946289\n","Iteration 51800: Loss 0.5562064554335083\n","Iteration 51900: Loss 0.5545406010078267\n","Iteration 52000: Loss 0.5538519654780626\n","Iteration 52100: Loss 0.5542258605957031\n","Iteration 52200: Loss 0.5582565095901489\n","Iteration 52300: Loss 0.556907573209703\n","Iteration 52400: Loss 0.5606248460769653\n","Iteration 52500: Loss 0.5550897068752907\n","Iteration 52600: Loss 0.5552594961643219\n","Iteration 52700: Loss 0.5563776594161988\n","Iteration 52800: Loss 0.5535112047547475\n","Iteration 52900: Loss 0.5551886719226837\n","Iteration 53000: Loss 0.5531939988031983\n","Iteration 53100: Loss 0.5570748209953308\n","Iteration 53200: Loss 0.5563667716741562\n","Iteration 53300: Loss 0.5573743602752685\n","Iteration 53400: Loss 0.5554214097976684\n","Iteration 53500: Loss 0.557607667517662\n","Iteration 53600: Loss 0.5581058856149204\n","Iteration 53700: Loss 0.5598476704044267\n","Iteration 53800: Loss 0.5547164156913758\n","Iteration 53900: Loss 0.5563066618204117\n","Iteration 54000: Loss 0.5576943531751632\n","Iteration 54100: Loss 0.5539507537126541\n","Iteration 54200: Loss 0.5575032382965088\n","Iteration 54300: Loss 0.5577220876216888\n","Iteration 54400: Loss 0.5573946262359619\n","Iteration 54500: Loss 0.5620015575408935\n","Iteration 54600: Loss 0.5582784706115723\n","Iteration 54700: Loss 0.5574057997442462\n","Iteration 54800: Loss 0.5592552075862884\n","Iteration 54900: Loss 0.5570947701156139\n","Iteration 55000: Loss 0.5598588157653809\n","Iteration 55100: Loss 0.556032113647461\n","Iteration 55200: Loss 0.5643982717514038\n","Iteration 55300: Loss 0.5620572554290295\n","Iteration 55400: Loss 0.5589586409509182\n","Iteration 55500: Loss 0.5588549695968628\n","Iteration 55600: Loss 0.558844130730629\n","Iteration 55700: Loss 0.5559258617401123\n","Iteration 55800: Loss 0.558415434551239\n","Iteration 55900: Loss 0.5595262729048729\n","Iteration 56000: Loss 0.5570353618010878\n","Iteration 56100: Loss 0.5588261854171753\n","Iteration 56200: Loss 0.5582142770767212\n","Iteration 56300: Loss 0.5561054428100586\n","Iteration 56400: Loss 0.5616547997951508\n","Iteration 56500: Loss 0.5574038435637951\n","Iteration 56600: Loss 0.5579211058616638\n","Iteration 56700: Loss 0.5593634393692016\n","Iteration 56800: Loss 0.5551314580917358\n","Iteration 56900: Loss 0.5571919288426638\n","Iteration 57000: Loss 0.5584549527168274\n","Iteration 57100: Loss 0.5599323921203613\n","Iteration 57200: Loss 0.5608754683494568\n","Iteration 57300: Loss 0.5582488000392913\n","Iteration 57400: Loss 0.5594041143417359\n","Iteration 57500: Loss 0.5628249234199524\n","Iteration 57600: Loss 0.561716267933324\n","Iteration 57700: Loss 0.561736072921753\n","Iteration 57800: Loss 0.5588310074329377\n","Iteration 57900: Loss 0.5636401104450226\n","Iteration 58000: Loss 0.5618147557497024\n","Iteration 58100: Loss 0.5598591963380575\n","Iteration 58200: Loss 0.5613430267333984\n","Iteration 58300: Loss 0.5649287794023752\n","Iteration 58400: Loss 0.5592414608001709\n","Iteration 58500: Loss 0.5599957962036133\n","Iteration 58600: Loss 0.5596308188259601\n","Iteration 58700: Loss 0.5634667124497704\n","Iteration 58800: Loss 0.5609335083007813\n","Iteration 58900: Loss 0.5618871520519256\n","Iteration 59000: Loss 0.5638233334064484\n","Iteration 59100: Loss 0.5612891694599297\n","Iteration 59200: Loss 0.5618234950639307\n","Iteration 59300: Loss 0.5580214917659759\n","Iteration 59400: Loss 0.5586155345916748\n","Iteration 59500: Loss 0.5598462184727192\n","Iteration 59600: Loss 0.5588401889836183\n","Iteration 59700: Loss 0.5586082412719726\n","Iteration 59800: Loss 0.5610827108383178\n","Iteration 59900: Loss 0.5577319253921509\n","Iteration 60000: Loss 0.5607218364376575\n","Iteration 60100: Loss 0.5613879335403442\n","Iteration 60200: Loss 0.5604244421005249\n","Iteration 60300: Loss 0.5614478344917297\n","Iteration 60400: Loss 0.5608453385472297\n","Iteration 60500: Loss 0.5646178966522217\n","Iteration 60600: Loss 0.5614668564796448\n","Iteration 60700: Loss 0.5624137252807617\n","Iteration 60800: Loss 0.5609365637302399\n","Iteration 60900: Loss 0.5649866539001465\n","Iteration 61000: Loss 0.5627150151968002\n","Iteration 61100: Loss 0.5641006496429444\n","Iteration 61200: Loss 0.5604361879348755\n","Iteration 61300: Loss 0.5605848062515258\n","Iteration 61400: Loss 0.562059778046608\n","Iteration 61500: Loss 0.5634095566272735\n","Iteration 61600: Loss 0.5614270502090454\n","Iteration 61700: Loss 0.5579226108551025\n","Iteration 61800: Loss 0.5689959896663785\n","Iteration 61900: Loss 0.564208385181427\n","Iteration 62000: Loss 0.5633258382797242\n","Iteration 62100: Loss 0.5665547562599182\n","Iteration 62200: Loss 0.5616005803108215\n","Iteration 62300: Loss 0.5633279589653015\n","Iteration 62400: Loss 0.5633191075325013\n","Iteration 62500: Loss 0.5643710374832154\n","Iteration 62600: Loss 0.5632854816436768\n","Iteration 62700: Loss 0.5641489631652832\n","Iteration 62800: Loss 0.5683562457084655\n","Iteration 62900: Loss 0.5617304826736242\n","Iteration 63000: Loss 0.5608346871376038\n","Iteration 63100: Loss 0.5632545276641846\n","Iteration 63200: Loss 0.5635644577383995\n","Iteration 63300: Loss 0.5673299659729004\n","Iteration 63400: Loss 0.5682883457183838\n","Iteration 63500: Loss 0.5672090514089912\n","Iteration 63600: Loss 0.5675808345794677\n","Iteration 63700: Loss 0.5674955906152725\n","Iteration 63800: Loss 0.5656965370178223\n","Iteration 63900: Loss 0.5619355062628165\n","Iteration 64000: Loss 0.5670599194765091\n"]}],"execution_count":78},{"cell_type":"code","source":"model.eval()\ntest_loader = DataLoader([*zip(test_images, test_labels)], batch_size=batch_size, shuffle=False)\ncorrect = 0\nfor batch in test_loader:\n  features, target = batch[:-1], batch[-1]\n  features = features[0].to(device)\n  target = target.to(device)\n  outputs = model(features)\n  correct += torch.where(torch.argmax(torch.softmax(outputs, dim=1), dim=1)==target, 1, 0).sum()\nprint((correct/test_images.shape[0])*100)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-648IiXU-yt3","outputId":"af9ff984-fce4-47d6-a6a0-768c675ccb16"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(89.4800, device='cuda:0')\n"]}],"execution_count":79},{"cell_type":"code","source":"del model, features, target","metadata":{"id":"8Sm14JEmAxi-"},"outputs":[],"execution_count":80},{"cell_type":"code","source":"\nclass ResidualBlock(nn.Module):\n\n  def __init__(self, in_channels, out_channels, kernel, padding1, padding2, stride):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.upsampling = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride)\n\n    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel, padding=padding1, stride=stride)\n    self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel, padding=padding2, stride=1)\n    self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n    self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n\n    nn.init.normal_(self.upsampling.weight, 0, (2/32)**0.5)\n    nn.init.normal_(self.conv1.weight, 0, (2/32)**0.5)\n    nn.init.normal_(self.conv2.weight, 0, (2/32)**0.5)\n\n    nn.init.zeros_(self.upsampling.bias)\n    nn.init.zeros_(self.conv1.bias)\n    nn.init.zeros_(self.conv2.bias)\n\n  def forward(self, x):\n    if self.in_channels != self.out_channels:\n      x_skip = self.upsampling(x)\n    else:\n      x_skip = x\n\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = nn.functional.relu(x)\n    x = self.conv2(x)\n    x = x + x_skip\n    x = self.bn2(x)\n    x = nn.functional.relu(x)\n    return x\n\n\n\nclass ResNet32(nn.Module):\n\n  def __init__(self):\n    super().__init__()\n    self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n\n    self.res_block_16_1 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_2 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_3 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_4 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_5 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_6 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_7 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_8 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_9 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_10 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_32_1 = ResidualBlock(in_channels=16, out_channels=32, kernel=3, padding1=1, padding2=1, stride=2)\n    self.res_block_32_2 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_3 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_4 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_5 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_6 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_7 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_8 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_9 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_10 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_1 = ResidualBlock(in_channels=32, out_channels=64, kernel=3, padding1=1, padding2=1, stride=2)\n    self.res_block_64_2 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_3 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_4 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_5 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_6 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_7 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_8 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_9 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_10 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n\n    self.global_pooling = nn.AvgPool2d(kernel_size=8)\n    self.fc = nn.Linear(64, 10)\n\n    nn.init.normal_(self.conv1.weight, 0, (2/32)**0.5)\n    nn.init.normal_(self.fc.weight, 0, (2/32)**0.5)\n\n    nn.init.zeros_(self.conv1.bias)\n    nn.init.zeros_(self.fc.bias)\n\n\n  def forward(self, x):\n    x = self.conv1(x)\n    x = self.res_block_16_1(x)\n    x = self.res_block_16_2(x)\n    x = self.res_block_16_3(x)\n    x = self.res_block_16_4(x)\n    x = self.res_block_16_5(x)\n    x = self.res_block_16_6(x)\n    x = self.res_block_16_7(x)\n    x = self.res_block_16_8(x)\n    x = self.res_block_16_9(x)\n    x = self.res_block_16_10(x)\n    x = self.res_block_32_1(x)\n    x = self.res_block_32_2(x)\n    x = self.res_block_32_3(x)\n    x = self.res_block_32_4(x)\n    x = self.res_block_32_5(x)\n    x = self.res_block_32_6(x)\n    x = self.res_block_32_7(x)\n    x = self.res_block_32_8(x)\n    x = self.res_block_32_9(x)\n    x = self.res_block_32_10(x)\n    x = self.res_block_64_1(x)\n    x = self.res_block_64_2(x)\n    x = self.res_block_64_3(x)\n    x = self.res_block_64_4(x)\n    x = self.res_block_64_5(x)\n    x = self.res_block_64_6(x)\n    x = self.res_block_64_7(x)\n    x = self.res_block_64_8(x)\n    x = self.res_block_64_9(x)\n    x = self.res_block_64_10(x)\n    x = self.global_pooling(x)\n    x = torch.flatten(x, 1, -1)\n    x = self.fc(x)\n    return x","metadata":{"id":"FlSjVx0IAz3e","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:23:02.292316Z","iopub.execute_input":"2025-05-13T03:23:02.292514Z","iopub.status.idle":"2025-05-13T03:23:02.314244Z","shell.execute_reply.started":"2025-05-13T03:23:02.292500Z","shell.execute_reply":"2025-05-13T03:23:02.313587Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"weight_decay = 0.0001\nmomentum = 0.9\nlr = 0.1\nbatch_size = 128\niterations = 0\nover=False\n\ntrain_batches = DataLoader([*zip(X_train, y_train)], batch_size=batch_size, shuffle=True)\nval_batches = DataLoader([*zip(X_val, y_val)], batch_size=batch_size, shuffle=True)\nloss_fn = nn.CrossEntropyLoss()\nval_loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\nmodel = ResNet32().to(device)\noptimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n\nwhile True:\n\n  for batch in train_batches:\n    model.train()\n    iterations += 1\n    optimizer.zero_grad()\n    features, target = batch[:-1], batch[-1]\n    features = features[0].to(device)\n    target = target.to(device)\n    outputs = model(features)\n    perte = loss_fn(outputs, target)\n    perte.backward()\n    optimizer.step()\n\n    if iterations % 100 == 0:\n      model.eval()\n      total_loss = 0\n      for batch in val_batches:\n        features, target = batch[:-1], batch[-1]\n        features = features[0].to(device)\n        target = target.to(device)\n        outputs = model(features)\n        perte = val_loss_fn(outputs, target)\n        total_loss += perte.item()\n      print(f\"Iteration {iterations}: Loss {total_loss/X_val.shape[0]}\")\n\n    match iterations:\n      case 32000:\n        lr /= 10\n        optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n      case 48000:\n        lr /= 10\n        optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n      case 64000:\n        over=True\n        break\n      case _:\n        continue\n\n\n\n\n  if over:\n    break\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_d5BNOgBpGr","outputId":"30c1cc78-d73d-4674-bd56-923c04332940","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:23:02.315063Z","iopub.execute_input":"2025-05-13T03:23:02.315379Z","iopub.status.idle":"2025-05-13T05:25:16.070048Z","shell.execute_reply.started":"2025-05-13T03:23:02.315362Z","shell.execute_reply":"2025-05-13T05:25:16.069310Z"}},"outputs":[{"name":"stdout","text":"Iteration 100: Loss 2.3226703525543213\nIteration 200: Loss 2.3050545318603515\nIteration 300: Loss 2.2973716629028322\nIteration 400: Loss 2.290064970397949\nIteration 500: Loss 2.2853515075683593\nIteration 600: Loss 2.2823936752319334\nIteration 700: Loss 2.279480103302002\nIteration 800: Loss 2.262299355697632\nIteration 900: Loss 2.2617228031158447\nIteration 1000: Loss 2.294735417938232\nIteration 1100: Loss 2.1927947784423827\nIteration 1200: Loss 2.222826135635376\nIteration 1300: Loss 2.1830554988861084\nIteration 1400: Loss 2.273524047470093\nIteration 1500: Loss 2.0837497257232664\nIteration 1600: Loss 2.150034883880615\nIteration 1700: Loss 2.033199182891846\nIteration 1800: Loss 1.9862348648071289\nIteration 1900: Loss 1.9213703172683716\nIteration 2000: Loss 2.100653741455078\nIteration 2100: Loss 2.0878962371826173\nIteration 2200: Loss 1.8215091533660888\nIteration 2300: Loss 1.7649912139892578\nIteration 2400: Loss 2.0041395950317384\nIteration 2500: Loss 1.793077935028076\nIteration 2600: Loss 1.7395268241882325\nIteration 2700: Loss 2.016572029685974\nIteration 2800: Loss 1.7148621231079102\nIteration 2900: Loss 1.66755335521698\nIteration 3000: Loss 1.6695823133468628\nIteration 3100: Loss 1.6894021404266357\nIteration 3200: Loss 1.689312914276123\nIteration 3300: Loss 1.8682578628540039\nIteration 3400: Loss 1.8391364349365233\nIteration 3500: Loss 1.5500937019348144\nIteration 3600: Loss 1.602295686340332\nIteration 3700: Loss 1.5383869688034058\nIteration 3800: Loss 2.520223853302002\nIteration 3900: Loss 1.540311994934082\nIteration 4000: Loss 1.6705942459106444\nIteration 4100: Loss 1.4554713243484496\nIteration 4200: Loss 1.4883858255386353\nIteration 4300: Loss 1.5150905801773071\nIteration 4400: Loss 1.4004604740142823\nIteration 4500: Loss 1.4778866163253783\nIteration 4600: Loss 1.5519343254089355\nIteration 4700: Loss 1.7234332139968873\nIteration 4800: Loss 1.3813015180587769\nIteration 4900: Loss 1.5530219562530518\nIteration 5000: Loss 1.7197633884429933\nIteration 5100: Loss 1.485469415283203\nIteration 5200: Loss 1.654168625831604\nIteration 5300: Loss 1.320840540790558\nIteration 5400: Loss 1.4896122707366943\nIteration 5500: Loss 1.5064170555114746\nIteration 5600: Loss 1.2561912300109863\nIteration 5700: Loss 1.4501699338912963\nIteration 5800: Loss 1.4360083135604857\nIteration 5900: Loss 1.2028597465515136\nIteration 6000: Loss 1.4021264617919922\nIteration 6100: Loss 1.370748963737488\nIteration 6200: Loss 1.2137813764572143\nIteration 6300: Loss 1.4255659189224243\nIteration 6400: Loss 1.4191366958618163\nIteration 6500: Loss 1.1997593466758727\nIteration 6600: Loss 1.1000024395942687\nIteration 6700: Loss 1.192017688369751\nIteration 6800: Loss 1.4590567462921142\nIteration 6900: Loss 1.2512313230514527\nIteration 7000: Loss 1.1776264904022218\nIteration 7100: Loss 1.085818168258667\nIteration 7200: Loss 1.1084916935920714\nIteration 7300: Loss 1.172490690612793\nIteration 7400: Loss 1.269453281021118\nIteration 7500: Loss 1.2689649227142334\nIteration 7600: Loss 1.2678247650146484\nIteration 7700: Loss 1.555201190185547\nIteration 7800: Loss 1.1636326484680175\nIteration 7900: Loss 1.2540439611434937\nIteration 8000: Loss 1.2723579999923706\nIteration 8100: Loss 1.3186159856796265\nIteration 8200: Loss 1.2298431060791015\nIteration 8300: Loss 1.0567508192062378\nIteration 8400: Loss 1.3071648250579835\nIteration 8500: Loss 0.9868333784103394\nIteration 8600: Loss 0.9964038376808166\nIteration 8700: Loss 1.0288808108329772\nIteration 8800: Loss 1.0309443618774414\nIteration 8900: Loss 1.0833720395088196\nIteration 9000: Loss 1.0124814593315123\nIteration 9100: Loss 1.0984351985931395\nIteration 9200: Loss 1.0701511814117433\nIteration 9300: Loss 1.0831081068992614\nIteration 9400: Loss 0.9950676142215729\nIteration 9500: Loss 1.3179341865539551\nIteration 9600: Loss 1.1380836604118347\nIteration 9700: Loss 0.9691295555114746\nIteration 9800: Loss 0.9221086408615112\nIteration 9900: Loss 0.9424292148590088\nIteration 10000: Loss 1.060469430923462\nIteration 10100: Loss 1.094118430328369\nIteration 10200: Loss 0.9629480855941772\nIteration 10300: Loss 1.4547704391479492\nIteration 10400: Loss 1.118767843914032\nIteration 10500: Loss 0.932364708328247\nIteration 10600: Loss 0.9672411346435547\nIteration 10700: Loss 0.9193399409294128\nIteration 10800: Loss 0.9806821245193481\nIteration 10900: Loss 0.9004050448417663\nIteration 11000: Loss 1.1094410623550415\nIteration 11100: Loss 0.9506431465148926\nIteration 11200: Loss 0.9139683065414429\nIteration 11300: Loss 0.9485330520629883\nIteration 11400: Loss 1.0506407743930817\nIteration 11500: Loss 1.005797862625122\nIteration 11600: Loss 0.9869535627365112\nIteration 11700: Loss 1.351085643196106\nIteration 11800: Loss 0.9458888788223266\nIteration 11900: Loss 0.8883144134521485\nIteration 12000: Loss 1.2132713729858398\nIteration 12100: Loss 0.8579104545593261\nIteration 12200: Loss 1.1183548931121827\nIteration 12300: Loss 0.9416864332199096\nIteration 12400: Loss 0.9283052345275878\nIteration 12500: Loss 0.975160196685791\nIteration 12600: Loss 1.0964223775863648\nIteration 12700: Loss 0.8317848428726197\nIteration 12800: Loss 0.9910614846229553\nIteration 12900: Loss 0.8493864793777466\nIteration 13000: Loss 0.8713705659866333\nIteration 13100: Loss 1.0259522653579711\nIteration 13200: Loss 1.0436787704467774\nIteration 13300: Loss 0.9470894550323486\nIteration 13400: Loss 0.8274867031097413\nIteration 13500: Loss 0.8889659858703614\nIteration 13600: Loss 0.8405902495384217\nIteration 13700: Loss 0.8896945466518402\nIteration 13800: Loss 0.9573409289360046\nIteration 13900: Loss 0.8380412712097168\nIteration 14000: Loss 0.9839942131042481\nIteration 14100: Loss 0.868624095249176\nIteration 14200: Loss 1.0497953619718552\nIteration 14300: Loss 0.7691139671325684\nIteration 14400: Loss 0.8299435923099517\nIteration 14500: Loss 0.9215526678085327\nIteration 14600: Loss 0.827258532333374\nIteration 14700: Loss 1.0149214944839478\nIteration 14800: Loss 0.8896905698776245\nIteration 14900: Loss 0.917596467590332\nIteration 15000: Loss 0.8723768438339233\nIteration 15100: Loss 1.2485770919799806\nIteration 15200: Loss 0.976901965713501\nIteration 15300: Loss 1.0561149139404298\nIteration 15400: Loss 0.971990337562561\nIteration 15500: Loss 1.0099169027328492\nIteration 15600: Loss 0.9451636701583862\nIteration 15700: Loss 0.8219912091255188\nIteration 15800: Loss 0.8774104198455811\nIteration 15900: Loss 0.8546878564834595\nIteration 16000: Loss 0.7862087742805481\nIteration 16100: Loss 1.0421716897964477\nIteration 16200: Loss 0.8118761194467544\nIteration 16300: Loss 0.8315333751678466\nIteration 16400: Loss 0.859900319480896\nIteration 16500: Loss 0.8085885776519776\nIteration 16600: Loss 0.8452569036006927\nIteration 16700: Loss 0.8094756271362304\nIteration 16800: Loss 0.909744444656372\nIteration 16900: Loss 0.8999153529167175\nIteration 17000: Loss 0.9493589442729949\nIteration 17100: Loss 0.9159250607967376\nIteration 17200: Loss 0.8599592668533326\nIteration 17300: Loss 0.7875044877052307\nIteration 17400: Loss 1.221019449043274\nIteration 17500: Loss 0.8841196216583252\nIteration 17600: Loss 0.7469340679168701\nIteration 17700: Loss 0.7782514015197753\nIteration 17800: Loss 0.764391770362854\nIteration 17900: Loss 0.7403791735649109\nIteration 18000: Loss 0.9313449831485748\nIteration 18100: Loss 0.7902170561790466\nIteration 18200: Loss 0.8418544076919555\nIteration 18300: Loss 0.9473534744262695\nIteration 18400: Loss 0.8316936516761779\nIteration 18500: Loss 0.8295623070716858\nIteration 18600: Loss 0.9618912195205689\nIteration 18700: Loss 0.806197003364563\nIteration 18800: Loss 0.7901506093502044\nIteration 18900: Loss 0.8244200503587723\nIteration 19000: Loss 0.8964703454971313\nIteration 19100: Loss 0.9036940113067627\nIteration 19200: Loss 0.8018209617614747\nIteration 19300: Loss 0.7508963917732239\nIteration 19400: Loss 0.754941529083252\nIteration 19500: Loss 0.9210829033851623\nIteration 19600: Loss 0.8060966632843017\nIteration 19700: Loss 0.9543727914810181\nIteration 19800: Loss 0.848723459815979\nIteration 19900: Loss 0.770307166481018\nIteration 20000: Loss 0.7674474945068359\nIteration 20100: Loss 0.7849923141479492\nIteration 20200: Loss 0.8497640550613403\nIteration 20300: Loss 0.8680777627944947\nIteration 20400: Loss 0.928385384273529\nIteration 20500: Loss 0.8384116691350937\nIteration 20600: Loss 0.8363419237136841\nIteration 20700: Loss 0.8976745893478394\nIteration 20800: Loss 0.679874661064148\nIteration 20900: Loss 1.0476040409088134\nIteration 21000: Loss 0.8806397575378418\nIteration 21100: Loss 0.7745447957992554\nIteration 21200: Loss 0.7171079568862915\nIteration 21300: Loss 1.093273327255249\nIteration 21400: Loss 0.9049418407440185\nIteration 21500: Loss 0.8946790714263916\nIteration 21600: Loss 0.7862418403625488\nIteration 21700: Loss 0.8240071775436402\nIteration 21800: Loss 1.1713896591186523\nIteration 21900: Loss 0.776243874168396\nIteration 22000: Loss 0.7609736285209656\nIteration 22100: Loss 0.8786995594978333\nIteration 22200: Loss 0.6496351554870605\nIteration 22300: Loss 0.760481672668457\nIteration 22400: Loss 0.7386069929122925\nIteration 22500: Loss 0.7783388710021972\nIteration 22600: Loss 0.8433347568511963\nIteration 22700: Loss 0.7838516133308411\nIteration 22800: Loss 0.783526987361908\nIteration 22900: Loss 0.8011132021427154\nIteration 23000: Loss 0.7973832143783569\nIteration 23100: Loss 0.7660253093719482\nIteration 23200: Loss 0.9847578792572022\nIteration 23300: Loss 0.9477209503173828\nIteration 23400: Loss 0.789724947977066\nIteration 23500: Loss 0.842221623134613\nIteration 23600: Loss 0.8077764198303222\nIteration 23700: Loss 0.8146035577774048\nIteration 23800: Loss 0.7427620347976684\nIteration 23900: Loss 0.7115707055568695\nIteration 24000: Loss 0.7206364133834838\nIteration 24100: Loss 0.8322952875137329\nIteration 24200: Loss 0.8400582071304321\nIteration 24300: Loss 0.7666424320697784\nIteration 24400: Loss 0.9055981424331665\nIteration 24500: Loss 0.8000556631088257\nIteration 24600: Loss 0.7195163499832153\nIteration 24700: Loss 0.7020460456848144\nIteration 24800: Loss 0.7626532245635986\nIteration 24900: Loss 0.7463741807937622\nIteration 25000: Loss 0.763869484603405\nIteration 25100: Loss 0.8657368870735168\nIteration 25200: Loss 0.7837007469177246\nIteration 25300: Loss 0.828464822101593\nIteration 25400: Loss 0.8250678825378418\nIteration 25500: Loss 0.8254857549667358\nIteration 25600: Loss 0.7465451343059539\nIteration 25700: Loss 0.8811522567749024\nIteration 25800: Loss 0.7443871191978455\nIteration 25900: Loss 1.1807006507873534\nIteration 26000: Loss 0.8173923764228821\nIteration 26100: Loss 2.112111085128784\nIteration 26200: Loss 0.8490184604644775\nIteration 26300: Loss 0.7624020454406738\nIteration 26400: Loss 0.8479680668830871\nIteration 26500: Loss 0.7333901381492615\nIteration 26600: Loss 0.8275399875640869\nIteration 26700: Loss 0.8373283977508545\nIteration 26800: Loss 0.7702615768432617\nIteration 26900: Loss 0.7531476661682129\nIteration 27000: Loss 0.7505854518890381\nIteration 27100: Loss 0.7191068784713746\nIteration 27200: Loss 1.000595415878296\nIteration 27300: Loss 0.8439702791213989\nIteration 27400: Loss 0.7640820113956929\nIteration 27500: Loss 0.8592392614364625\nIteration 27600: Loss 0.6985311643600464\nIteration 27700: Loss 0.826133228302002\nIteration 27800: Loss 0.728329145526886\nIteration 27900: Loss 0.712995107793808\nIteration 28000: Loss 0.8201600116729736\nIteration 28100: Loss 0.8619875572204589\nIteration 28200: Loss 0.8750673403739929\nIteration 28300: Loss 0.8125542346954345\nIteration 28400: Loss 0.7972204447746277\nIteration 28500: Loss 0.788245867729187\nIteration 28600: Loss 0.7204993844985962\nIteration 28700: Loss 0.7055322652578354\nIteration 28800: Loss 0.7071633167266845\nIteration 28900: Loss 0.706122943687439\nIteration 29000: Loss 0.6995087998390198\nIteration 29100: Loss 0.8023249027252197\nIteration 29200: Loss 1.185958883666992\nIteration 29300: Loss 0.8147653633117676\nIteration 29400: Loss 0.7381587839126587\nIteration 29500: Loss 0.6922930522918701\nIteration 29600: Loss 0.9164462219238281\nIteration 29700: Loss 0.6821900665283203\nIteration 29800: Loss 3.070093253326416\nIteration 29900: Loss 0.6495919904708862\nIteration 30000: Loss 0.8446590837001801\nIteration 30100: Loss 0.7454236459732055\nIteration 30200: Loss 0.7993285215377808\nIteration 30300: Loss 0.7256868851661682\nIteration 30400: Loss 0.7601762699127197\nIteration 30500: Loss 0.7078290201187134\nIteration 30600: Loss 0.8610975350379944\nIteration 30700: Loss 0.8063721775054932\nIteration 30800: Loss 0.7330955745697022\nIteration 30900: Loss 0.7672830657958984\nIteration 31000: Loss 0.8223940137386322\nIteration 31100: Loss 0.70007664270401\nIteration 31200: Loss 0.7206764652729034\nIteration 31300: Loss 0.7711718034267425\nIteration 31400: Loss 0.7580169183731079\nIteration 31500: Loss 0.6924961135864258\nIteration 31600: Loss 0.7758499549865723\nIteration 31700: Loss 0.804554074382782\nIteration 31800: Loss 0.7573097249031067\nIteration 31900: Loss 0.754651683807373\nIteration 32000: Loss 0.7162815633773804\nIteration 32100: Loss 0.5557743070602417\nIteration 32200: Loss 0.5279265070438385\nIteration 32300: Loss 0.5234139163970948\nIteration 32400: Loss 0.5219601306915284\nIteration 32500: Loss 0.5226000601053238\nIteration 32600: Loss 0.5318636552810669\nIteration 32700: Loss 0.5313139116287231\nIteration 32800: Loss 0.5390799019813538\nIteration 32900: Loss 0.535541800403595\nIteration 33000: Loss 0.5383888896942138\nIteration 33100: Loss 0.54078094997406\nIteration 33200: Loss 0.5452007507324219\nIteration 33300: Loss 0.5365552391052246\nIteration 33400: Loss 0.5405498186759651\nIteration 33500: Loss 0.5425231952905655\nIteration 33600: Loss 0.5575658851832151\nIteration 33700: Loss 0.5585335116922855\nIteration 33800: Loss 0.5683990337371826\nIteration 33900: Loss 0.5714978982925415\nIteration 34000: Loss 0.5721271934509278\nIteration 34100: Loss 0.5749194145202636\nIteration 34200: Loss 0.5765278932571412\nIteration 34300: Loss 0.5782587209701538\nIteration 34400: Loss 0.5695434410095215\nIteration 34500: Loss 0.5758531267166138\nIteration 34600: Loss 0.5798355214118958\nIteration 34700: Loss 0.5839417249679565\nIteration 34800: Loss 0.5929110586166382\nIteration 34900: Loss 0.5908360323905945\nIteration 35000: Loss 0.5853561588287354\nIteration 35100: Loss 0.5746965006351471\nIteration 35200: Loss 0.5995886024475098\nIteration 35300: Loss 0.599709448504448\nIteration 35400: Loss 0.608015931558609\nIteration 35500: Loss 0.6090432559013367\nIteration 35600: Loss 0.6111338915228843\nIteration 35700: Loss 0.6217965347290039\nIteration 35800: Loss 0.6238221342086792\nIteration 35900: Loss 0.6312381972670555\nIteration 36000: Loss 0.6351449721373152\nIteration 36100: Loss 0.6310389510631561\nIteration 36200: Loss 0.6359094000220299\nIteration 36300: Loss 0.6339859243392945\nIteration 36400: Loss 0.616839134979248\nIteration 36500: Loss 0.6337989313840866\nIteration 36600: Loss 0.6491789912700653\nIteration 36700: Loss 0.6317792379498481\nIteration 36800: Loss 0.6392846873283387\nIteration 36900: Loss 0.637728862953186\nIteration 37000: Loss 0.6388990378856659\nIteration 37100: Loss 0.6689036575317383\nIteration 37200: Loss 0.6940969162464142\nIteration 37300: Loss 0.6733948732376098\nIteration 37400: Loss 0.6727192422866821\nIteration 37500: Loss 0.685303489112854\nIteration 37600: Loss 0.6930278041124344\nIteration 37700: Loss 0.6933962453842163\nIteration 37800: Loss 0.719710402405262\nIteration 37900: Loss 0.6986500179290771\nIteration 38000: Loss 0.6924968158721924\nIteration 38100: Loss 0.6971273279607296\nIteration 38200: Loss 0.7151943274021149\nIteration 38300: Loss 0.6947984700314701\nIteration 38400: Loss 0.7209549386590719\nIteration 38500: Loss 0.7129347680091858\nIteration 38600: Loss 0.7085648788452148\nIteration 38700: Loss 0.7068435869216919\nIteration 38800: Loss 0.7358737049102784\nIteration 38900: Loss 0.7244632961273193\nIteration 39000: Loss 0.7397116287231446\nIteration 39100: Loss 0.7500099472999573\nIteration 39200: Loss 0.7661171686172485\nIteration 39300: Loss 0.7545744386672973\nIteration 39400: Loss 0.776143249130249\nIteration 39500: Loss 0.7679164240002632\nIteration 39600: Loss 0.748094107055664\nIteration 39700: Loss 0.7794730126857757\nIteration 39800: Loss 0.7647551735460758\nIteration 39900: Loss 0.7529627014398574\nIteration 40000: Loss 0.7672225501060486\nIteration 40100: Loss 0.7776870803833008\nIteration 40200: Loss 0.7462542649269104\nIteration 40300: Loss 0.7614529122106731\nIteration 40400: Loss 0.7753579496383667\nIteration 40500: Loss 0.7591074394702911\nIteration 40600: Loss 0.7914231138229371\nIteration 40700: Loss 0.7851998863479123\nIteration 40800: Loss 0.8000913421750069\nIteration 40900: Loss 0.7903762773990631\nIteration 41000: Loss 0.824154812335968\nIteration 41100: Loss 0.8285530521392822\nIteration 41200: Loss 0.8504310883760452\nIteration 41300: Loss 0.8180290420532227\nIteration 41400: Loss 0.8039449762344361\nIteration 41500: Loss 0.8168949857950211\nIteration 41600: Loss 0.832160365100205\nIteration 41700: Loss 0.8414257309436798\nIteration 41800: Loss 0.8508324445724488\nIteration 41900: Loss 0.8366369425773621\nIteration 42000: Loss 0.8380628205776215\nIteration 42100: Loss 0.835061782836914\nIteration 42200: Loss 0.8270790070474148\nIteration 42300: Loss 0.82003217420578\nIteration 42400: Loss 0.8486527070999146\nIteration 42500: Loss 0.8362751314163208\nIteration 42600: Loss 0.8688547882080078\nIteration 42700: Loss 0.8652350910186768\nIteration 42800: Loss 0.8412906898498536\nIteration 42900: Loss 0.876387409125641\nIteration 43000: Loss 0.875750595998764\nIteration 43100: Loss 0.8575120170593262\nIteration 43200: Loss 0.8700909500658512\nIteration 43300: Loss 0.8668288089752197\nIteration 43400: Loss 0.8928770852088929\nIteration 43500: Loss 0.9039365797042846\nIteration 43600: Loss 0.8719236539840698\nIteration 43700: Loss 0.9008956153899431\nIteration 43800: Loss 0.876041503226757\nIteration 43900: Loss 0.8612188278198242\nIteration 44000: Loss 0.8498130413472652\nIteration 44100: Loss 0.8575912536621094\nIteration 44200: Loss 0.8648563976287842\nIteration 44300: Loss 0.8694081069946289\nIteration 44400: Loss 0.8932092441558838\nIteration 44500: Loss 0.8986673168182373\nIteration 44600: Loss 0.9137818425416946\nIteration 44700: Loss 0.903565361738205\nIteration 44800: Loss 0.8937314316213131\nIteration 44900: Loss 0.9074847259521485\nIteration 45000: Loss 0.9006486959934235\nIteration 45100: Loss 0.9063805970594286\nIteration 45200: Loss 0.8965766702175141\nIteration 45300: Loss 0.9643711632728577\nIteration 45400: Loss 0.9538281093597412\nIteration 45500: Loss 0.9028741647720336\nIteration 45600: Loss 0.9071762809753418\nIteration 45700: Loss 0.8878254272460937\nIteration 45800: Loss 0.9385264362335205\nIteration 45900: Loss 0.9326853000104427\nIteration 46000: Loss 0.9590180854797363\nIteration 46100: Loss 0.9489840437889099\nIteration 46200: Loss 0.9387558458328247\nIteration 46300: Loss 0.9241528064727783\nIteration 46400: Loss 0.9367350022077561\nIteration 46500: Loss 0.9132619274139404\nIteration 46600: Loss 0.9482766498565673\nIteration 46700: Loss 0.9462578168928624\nIteration 46800: Loss 0.9313581674575806\nIteration 46900: Loss 0.9599846490859986\nIteration 47000: Loss 0.9413484120488167\nIteration 47100: Loss 0.9408142895698547\nIteration 47200: Loss 0.9474985595703125\nIteration 47300: Loss 0.9308476221084595\nIteration 47400: Loss 0.9472934735298156\nIteration 47500: Loss 0.9330612815856933\nIteration 47600: Loss 0.958971715593338\nIteration 47700: Loss 0.9562860261917114\nIteration 47800: Loss 0.9529582940042018\nIteration 47900: Loss 0.9653091911315917\nIteration 48000: Loss 0.9385893249511719\nIteration 48100: Loss 0.9224575184758752\nIteration 48200: Loss 0.9206735559463501\nIteration 48300: Loss 0.9261971606492996\nIteration 48400: Loss 0.9250867023944854\nIteration 48500: Loss 0.9226363225846551\nIteration 48600: Loss 0.9286920148968697\nIteration 48700: Loss 0.9277480677074753\nIteration 48800: Loss 0.9278361623287201\nIteration 48900: Loss 0.9290591115951539\nIteration 49000: Loss 0.9302266618728637\nIteration 49100: Loss 0.9318870433807374\nIteration 49200: Loss 0.9346994557380677\nIteration 49300: Loss 0.9310445041656494\nIteration 49400: Loss 0.9387661642817781\nIteration 49500: Loss 0.9416068641662597\nIteration 49600: Loss 0.945984957408905\nIteration 49700: Loss 0.9411222101211548\nIteration 49800: Loss 0.9423915981292724\nIteration 49900: Loss 0.946879035949707\nIteration 50000: Loss 0.9479474277496338\nIteration 50100: Loss 0.9526013916037976\nIteration 50200: Loss 0.9523166625035229\nIteration 50300: Loss 0.957648053586483\nIteration 50400: Loss 0.9419889165401458\nIteration 50500: Loss 0.9464159912109374\nIteration 50600: Loss 0.9557519809387625\nIteration 50700: Loss 0.9540075534820557\nIteration 50800: Loss 0.9567820741653442\nIteration 50900: Loss 0.9598115519762039\nIteration 51000: Loss 0.9592737804412842\nIteration 51100: Loss 0.9644612964630127\nIteration 51200: Loss 0.9656566251754761\nIteration 51300: Loss 0.9732181879043579\nIteration 51400: Loss 0.9742702894210815\nIteration 51500: Loss 0.9642296691894531\nIteration 51600: Loss 0.9670075407028198\nIteration 51700: Loss 0.9741336500644684\nIteration 51800: Loss 0.963925869357586\nIteration 51900: Loss 0.9764623053550721\nIteration 52000: Loss 0.9813917421007529\nIteration 52100: Loss 0.9761037006378174\nIteration 52200: Loss 0.9735129266738891\nIteration 52300: Loss 0.9713305329024792\nIteration 52400: Loss 0.9800689412707463\nIteration 52500: Loss 0.9764011930465698\nIteration 52600: Loss 0.9835109046936035\nIteration 52700: Loss 0.9820003742218018\nIteration 52800: Loss 0.9741089304924011\nIteration 52900: Loss 0.9779815015792847\nIteration 53000: Loss 0.9839353054046631\nIteration 53100: Loss 0.9823101006507874\nIteration 53200: Loss 0.9849158206939698\nIteration 53300: Loss 0.9897295523308217\nIteration 53400: Loss 0.9833989954948426\nIteration 53500: Loss 0.9865220220565796\nIteration 53600: Loss 0.9871734972953796\nIteration 53700: Loss 0.995505842590332\nIteration 53800: Loss 0.9879592188835145\nIteration 53900: Loss 0.9940773421287536\nIteration 54000: Loss 0.9918847509384155\nIteration 54100: Loss 0.9995979539871216\nIteration 54200: Loss 0.9962530181884766\nIteration 54300: Loss 0.9928161529950797\nIteration 54400: Loss 1.0061234670639039\nIteration 54500: Loss 0.9965764705657959\nIteration 54600: Loss 1.0050926878563362\nIteration 54700: Loss 0.9996764901161194\nIteration 54800: Loss 0.9982286508560181\nIteration 54900: Loss 1.0029629517257213\nIteration 55000: Loss 0.9964339981079101\nIteration 55100: Loss 1.0002229335784911\nIteration 55200: Loss 0.9993397552490234\nIteration 55300: Loss 1.0026960106372833\nIteration 55400: Loss 1.0067817962646484\nIteration 55500: Loss 1.0052188669204711\nIteration 55600: Loss 1.003344309234619\nIteration 55700: Loss 1.0029629214286804\nIteration 55800: Loss 1.007477918624878\nIteration 55900: Loss 1.0040969911575317\nIteration 56000: Loss 1.0045006549835205\nIteration 56100: Loss 1.0047106075763703\nIteration 56200: Loss 1.0051310070857407\nIteration 56300: Loss 1.0138409225463867\nIteration 56400: Loss 1.0092565841674805\nIteration 56500: Loss 1.0102602272033692\nIteration 56600: Loss 1.0112476900100709\nIteration 56700: Loss 1.0128503719329833\nIteration 56800: Loss 1.01241497631073\nIteration 56900: Loss 1.015691707420349\nIteration 57000: Loss 1.0190125540886075\nIteration 57100: Loss 1.0227150297045708\nIteration 57200: Loss 1.0135963537216186\nIteration 57300: Loss 1.014679053016659\nIteration 57400: Loss 1.0194175880432128\nIteration 57500: Loss 1.0135694689750672\nIteration 57600: Loss 1.0157639937400817\nIteration 57700: Loss 1.0199225902557374\nIteration 57800: Loss 1.0158526708602906\nIteration 57900: Loss 1.0177349544525147\nIteration 58000: Loss 1.016300821685791\nIteration 58100: Loss 1.0208948244094849\nIteration 58200: Loss 1.0213273128509521\nIteration 58300: Loss 1.0173398845672608\nIteration 58400: Loss 1.0126092667579651\nIteration 58500: Loss 1.0193836044311524\nIteration 58600: Loss 1.0219922950744629\nIteration 58700: Loss 1.0241210203986615\nIteration 58800: Loss 1.024217305602506\nIteration 58900: Loss 1.0233806305408477\nIteration 59000: Loss 1.0213824760437011\nIteration 59100: Loss 1.029233416557312\nIteration 59200: Loss 1.0257264205932617\nIteration 59300: Loss 1.0328812925338746\nIteration 59400: Loss 1.0229406597137451\nIteration 59500: Loss 1.036165217590332\nIteration 59600: Loss 1.0309090034484862\nIteration 59700: Loss 1.035675081884861\nIteration 59800: Loss 1.0292051703639329\nIteration 59900: Loss 1.0293130226135254\nIteration 60000: Loss 1.032148508644104\nIteration 60100: Loss 1.0298126653909683\nIteration 60200: Loss 1.026633664894104\nIteration 60300: Loss 1.0292655167579652\nIteration 60400: Loss 1.0303416257858276\nIteration 60500: Loss 1.0271476999282836\nIteration 60600: Loss 1.0360671009048819\nIteration 60700: Loss 1.035261155796051\nIteration 60800: Loss 1.0351766916275025\nIteration 60900: Loss 1.0350138610839843\nIteration 61000: Loss 1.0306063245773316\nIteration 61100: Loss 1.0361127740196883\nIteration 61200: Loss 1.0355075733184815\nIteration 61300: Loss 1.0375674991607666\nIteration 61400: Loss 1.0357939699172973\nIteration 61500: Loss 1.0371387613296508\nIteration 61600: Loss 1.0358108222961426\nIteration 61700: Loss 1.0373069480895996\nIteration 61800: Loss 1.0397976202964783\nIteration 61900: Loss 1.0391416820526123\nIteration 62000: Loss 1.0400209260940552\nIteration 62100: Loss 1.0408118059158324\nIteration 62200: Loss 1.0434293236732484\nIteration 62300: Loss 1.0439809720993043\nIteration 62400: Loss 1.0559733561515807\nIteration 62500: Loss 1.0501903470635414\nIteration 62600: Loss 1.0444333465576172\nIteration 62700: Loss 1.0449481782913208\nIteration 62800: Loss 1.0477567427158356\nIteration 62900: Loss 1.0474217443466187\nIteration 63000: Loss 1.039168083190918\nIteration 63100: Loss 1.046117374420166\nIteration 63200: Loss 1.0478642071723938\nIteration 63300: Loss 1.044748499941826\nIteration 63400: Loss 1.0496011157877743\nIteration 63500: Loss 1.0597632915496826\nIteration 63600: Loss 1.0464532758712768\nIteration 63700: Loss 1.0448659156799316\nIteration 63800: Loss 1.0481060613632203\nIteration 63900: Loss 1.0506269744873047\nIteration 64000: Loss 1.0479591733932496\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"model.eval()\ntest_loader = DataLoader([*zip(test_images, test_labels)], batch_size=batch_size, shuffle=False)\ncorrect = 0\nfor batch in test_loader:\n  features, target = batch[:-1], batch[-1]\n  features = features[0].to(device)\n  target = target.to(device)\n  outputs = model(features)\n  correct += torch.where(torch.argmax(torch.softmax(outputs, dim=1), dim=1)==target, 1, 0).sum()\nprint((correct/test_images.shape[0])*100)","metadata":{"id":"dD7wIjZFnvW1","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T05:25:16.070932Z","iopub.execute_input":"2025-05-13T05:25:16.071214Z","iopub.status.idle":"2025-05-13T05:25:18.088126Z","shell.execute_reply.started":"2025-05-13T05:25:16.071196Z","shell.execute_reply":"2025-05-13T05:25:18.087522Z"}},"outputs":[{"name":"stdout","text":"tensor(82.7400, device='cuda:0')\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"del model, features, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T05:25:18.113495Z","iopub.execute_input":"2025-05-13T05:25:18.113711Z","iopub.status.idle":"2025-05-13T05:25:18.131547Z","shell.execute_reply.started":"2025-05-13T05:25:18.113686Z","shell.execute_reply":"2025-05-13T05:25:18.130983Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\nclass ResidualBlock(nn.Module):\n\n  def __init__(self, in_channels, out_channels, kernel, padding1, padding2, stride):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.upsampling = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride)\n\n    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel, padding=padding1, stride=stride)\n    self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel, padding=padding2, stride=1)\n    self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n    self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n\n    nn.init.normal_(self.upsampling.weight, 0, (2/44)**0.5)\n    nn.init.normal_(self.conv1.weight, 0, (2/44)**0.5)\n    nn.init.normal_(self.conv2.weight, 0, (2/44)**0.5)\n\n    nn.init.zeros_(self.upsampling.bias)\n    nn.init.zeros_(self.conv1.bias)\n    nn.init.zeros_(self.conv2.bias)\n\n  def forward(self, x):\n    if self.in_channels != self.out_channels:\n      x_skip = self.upsampling(x)\n    else:\n      x_skip = x\n\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = nn.functional.relu(x)\n    x = self.conv2(x)\n    x = x + x_skip\n    x = self.bn2(x)\n    x = nn.functional.relu(x)\n    return x\n\n\n\nclass ResNet44(nn.Module):\n\n  def __init__(self):\n    super().__init__()\n    self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n\n    self.res_block_16_1 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_2 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_3 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_4 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_5 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_6 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_7 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_8 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_9 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_10 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_11 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_12 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_13 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)\n    self.res_block_16_14 = ResidualBlock(in_channels=16, out_channels=16, kernel=3, padding1=1,padding2=1,stride=1)  \n    self.res_block_32_1 = ResidualBlock(in_channels=16, out_channels=32, kernel=3, padding1=1, padding2=1, stride=2)\n    self.res_block_32_2 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_3 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_4 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_5 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_6 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_7 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_8 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_9 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_10 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_11 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_12 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_13 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_32_14 = ResidualBlock(in_channels=32, out_channels=32, kernel=3, padding1=1, padding2=1, stride=1)  \n    self.res_block_64_1 = ResidualBlock(in_channels=32, out_channels=64, kernel=3, padding1=1, padding2=1, stride=2)\n    self.res_block_64_2 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_3 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_4 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_5 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_6 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_7 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_8 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_9 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_10 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_11 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_12 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_13 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n    self.res_block_64_14 = ResidualBlock(in_channels=64, out_channels=64, kernel=3, padding1=1, padding2=1, stride=1)\n\n    self.global_pooling = nn.AvgPool2d(kernel_size=8)\n    self.fc = nn.Linear(64, 10)\n\n    nn.init.normal_(self.conv1.weight, 0, (2/44)**0.5)\n    nn.init.normal_(self.fc.weight, 0, (2/44)**0.5)\n\n    nn.init.zeros_(self.conv1.bias)\n    nn.init.zeros_(self.fc.bias)\n\n\n  def forward(self, x):\n    x = self.conv1(x)\n    x = self.res_block_16_1(x)\n    x = self.res_block_16_2(x)\n    x = self.res_block_16_3(x)\n    x = self.res_block_16_4(x)\n    x = self.res_block_16_5(x)\n    x = self.res_block_16_6(x)\n    x = self.res_block_16_7(x)\n    x = self.res_block_16_8(x)\n    x = self.res_block_16_9(x)\n    x = self.res_block_16_10(x)\n    x = self.res_block_16_11(x)\n    x = self.res_block_16_12(x)\n    x = self.res_block_16_13(x)\n    x = self.res_block_16_14(x)\n    x = self.res_block_32_1(x)\n    x = self.res_block_32_2(x)\n    x = self.res_block_32_3(x)\n    x = self.res_block_32_4(x)\n    x = self.res_block_32_5(x)\n    x = self.res_block_32_6(x)\n    x = self.res_block_32_7(x)\n    x = self.res_block_32_8(x)\n    x = self.res_block_32_9(x)\n    x = self.res_block_32_10(x)\n    x = self.res_block_32_11(x)\n    x = self.res_block_32_12(x)\n    x = self.res_block_32_13(x)\n    x = self.res_block_32_14(x)\n    x = self.res_block_64_1(x)\n    x = self.res_block_64_2(x)\n    x = self.res_block_64_3(x)\n    x = self.res_block_64_4(x)\n    x = self.res_block_64_5(x)\n    x = self.res_block_64_6(x)\n    x = self.res_block_64_7(x)\n    x = self.res_block_64_8(x)\n    x = self.res_block_64_9(x)\n    x = self.res_block_64_10(x)\n    x = self.res_block_64_11(x)\n    x = self.res_block_64_12(x)\n    x = self.res_block_64_13(x)\n    x = self.res_block_64_14(x)\n    x = self.global_pooling(x)\n    x = torch.flatten(x, 1, -1)\n    x = self.fc(x)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T05:25:18.089700Z","iopub.execute_input":"2025-05-13T05:25:18.089998Z","iopub.status.idle":"2025-05-13T05:25:18.112965Z","shell.execute_reply.started":"2025-05-13T05:25:18.089981Z","shell.execute_reply":"2025-05-13T05:25:18.112408Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"weight_decay = 0.0001\nmomentum = 0.9\nlr = 0.1\nbatch_size = 128\niterations = 0\nover=False\n\ntrain_batches = DataLoader([*zip(X_train, y_train)], batch_size=batch_size, shuffle=True)\nval_batches = DataLoader([*zip(X_val, y_val)], batch_size=batch_size, shuffle=True)\nloss_fn = nn.CrossEntropyLoss()\nval_loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\nmodel = ResNet44().to(device)\noptimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n\nwhile True:\n\n  for batch in train_batches:\n    model.train()\n    iterations += 1\n    optimizer.zero_grad()\n    features, target = batch[:-1], batch[-1]\n    features = features[0].to(device)\n    target = target.to(device)\n    outputs = model(features)\n    perte = loss_fn(outputs, target)\n    perte.backward()\n    optimizer.step()\n\n    if iterations % 100 == 0:\n      model.eval()\n      total_loss = 0\n      for batch in val_batches:\n        features, target = batch[:-1], batch[-1]\n        features = features[0].to(device)\n        target = target.to(device)\n        outputs = model(features)\n        perte = val_loss_fn(outputs, target)\n        total_loss += perte.item()\n      print(f\"Iteration {iterations}: Loss {total_loss/X_val.shape[0]}\")\n\n    match iterations:\n      case 32000:\n        lr /= 10\n        optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n      case 48000:\n        lr /= 10\n        optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n      case 64000:\n        over=True\n        break\n      case _:\n        continue\n\n\n\n\n  if over:\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T05:25:18.132276Z","iopub.execute_input":"2025-05-13T05:25:18.132512Z","iopub.status.idle":"2025-05-13T08:17:02.187782Z","shell.execute_reply.started":"2025-05-13T05:25:18.132493Z","shell.execute_reply":"2025-05-13T08:17:02.186970Z"}},"outputs":[{"name":"stdout","text":"Iteration 100: Loss 2.316779393386841\nIteration 200: Loss 2.305696852493286\nIteration 300: Loss 2.3121582534790037\nIteration 400: Loss 2.301698243713379\nIteration 500: Loss 2.300693458557129\nIteration 600: Loss 2.2969364921569824\nIteration 700: Loss 2.2968462966918946\nIteration 800: Loss 2.2976775859832763\nIteration 900: Loss 2.3028351707458494\nIteration 1000: Loss 2.2900081520080566\nIteration 1100: Loss 2.3550639694213866\nIteration 1200: Loss 2.285883277130127\nIteration 1300: Loss 2.3302780265808107\nIteration 1400: Loss 2.276705685806274\nIteration 1500: Loss 2.2733742637634275\nIteration 1600: Loss 2.2746206394195556\nIteration 1700: Loss 2.2775066928863525\nIteration 1800: Loss 2.27186792678833\nIteration 1900: Loss 2.2749424171447754\nIteration 2000: Loss 2.2688622138977053\nIteration 2100: Loss 2.266777786254883\nIteration 2200: Loss 2.2907829906463624\nIteration 2300: Loss 2.2770531482696534\nIteration 2400: Loss 2.272322298049927\nIteration 2500: Loss 2.266738856124878\nIteration 2600: Loss 2.272778411102295\nIteration 2700: Loss 2.2618846878051757\nIteration 2800: Loss 2.277972915267944\nIteration 2900: Loss 2.2921456649780274\nIteration 3000: Loss 2.229167205429077\nIteration 3100: Loss 2.2561776565551757\nIteration 3200: Loss 2.2666931045532226\nIteration 3300: Loss 2.222036053466797\nIteration 3400: Loss 2.2155678359985354\nIteration 3500: Loss 2.196811753463745\nIteration 3600: Loss 2.2659300510406495\nIteration 3700: Loss 2.243562023925781\nIteration 3800: Loss 2.3393787353515627\nIteration 3900: Loss 2.264249959564209\nIteration 4000: Loss 2.206091340255737\nIteration 4100: Loss 2.176033567047119\nIteration 4200: Loss 2.3819211074829103\nIteration 4300: Loss 2.377549835205078\nIteration 4400: Loss 2.477721109008789\nIteration 4500: Loss 2.2159803890228273\nIteration 4600: Loss 2.2360692863464355\nIteration 4700: Loss 2.1929069736480713\nIteration 4800: Loss 2.390814394760132\nIteration 4900: Loss 2.718893360900879\nIteration 5000: Loss 2.1610407314300537\nIteration 5100: Loss 2.4021162750244143\nIteration 5200: Loss 2.2194800186157226\nIteration 5300: Loss 2.1234608299255373\nIteration 5400: Loss 2.1979159454345703\nIteration 5500: Loss 2.343357927322388\nIteration 5600: Loss 2.293945753097534\nIteration 5700: Loss 2.2173280906677246\nIteration 5800: Loss 2.089141019439697\nIteration 5900: Loss 2.2336314918518068\nIteration 6000: Loss 2.08223957824707\nIteration 6100: Loss 2.0916645740509034\nIteration 6200: Loss 2.0863344234466554\nIteration 6300: Loss 2.364732038116455\nIteration 6400: Loss 2.2937684814453125\nIteration 6500: Loss 2.258914030456543\nIteration 6600: Loss 2.116744197845459\nIteration 6700: Loss 2.0332557416915895\nIteration 6800: Loss 2.0775882400512695\nIteration 6900: Loss 2.0129069843292235\nIteration 7000: Loss 2.2499944305419923\nIteration 7100: Loss 2.2218751277923583\nIteration 7200: Loss 2.016305438423157\nIteration 7300: Loss 2.0968181156158447\nIteration 7400: Loss 1.9734564273834228\nIteration 7500: Loss 1.9677850608825684\nIteration 7600: Loss 1.983381708908081\nIteration 7700: Loss 1.9706690357208252\nIteration 7800: Loss 2.0987032382965087\nIteration 7900: Loss 1.925471908569336\nIteration 8000: Loss 1.8930087677001952\nIteration 8100: Loss 1.91783568649292\nIteration 8200: Loss 2.1751704566955565\nIteration 8300: Loss 1.9046102432250978\nIteration 8400: Loss 1.9647043796539307\nIteration 8500: Loss 1.90685927028656\nIteration 8600: Loss 1.8768657396316528\nIteration 8700: Loss 1.8860551055908203\nIteration 8800: Loss 1.9501551057815552\nIteration 8900: Loss 1.9437470680236817\nIteration 9000: Loss 2.0085152130126955\nIteration 9100: Loss 1.8626676498413086\nIteration 9200: Loss 1.9082719612121581\nIteration 9300: Loss 1.8324567489624024\nIteration 9400: Loss 1.7956859428405763\nIteration 9500: Loss 1.8172011209487915\nIteration 9600: Loss 1.883447602081299\nIteration 9700: Loss 1.8705305158615113\nIteration 9800: Loss 1.856181881713867\nIteration 9900: Loss 1.7397780155181886\nIteration 10000: Loss 1.8752892152786256\nIteration 10100: Loss 1.754149415588379\nIteration 10200: Loss 1.7643613807678222\nIteration 10300: Loss 1.9845640884399414\nIteration 10400: Loss 1.9388196952819825\nIteration 10500: Loss 1.826175644683838\nIteration 10600: Loss 1.7430845041275025\nIteration 10700: Loss 1.7821191165924073\nIteration 10800: Loss 1.90983924407959\nIteration 10900: Loss 1.7644920112609863\nIteration 11000: Loss 1.7294305644989014\nIteration 11100: Loss 1.8074545219421387\nIteration 11200: Loss 1.7255101226806642\nIteration 11300: Loss 1.6693473428726195\nIteration 11400: Loss 1.7663947845458985\nIteration 11500: Loss 1.7398381366729736\nIteration 11600: Loss 1.6903613048553467\nIteration 11700: Loss 1.935859415435791\nIteration 11800: Loss 1.7495273319244384\nIteration 11900: Loss 1.7241571857452394\nIteration 12000: Loss 1.7801436748504638\nIteration 12100: Loss 1.6268598182678222\nIteration 12200: Loss 1.6631762420654297\nIteration 12300: Loss 1.719631803894043\nIteration 12400: Loss 1.7489436435699464\nIteration 12500: Loss 1.8072309642791748\nIteration 12600: Loss 1.9307445434570312\nIteration 12700: Loss 1.5918713722229003\nIteration 12800: Loss 1.7197049743652344\nIteration 12900: Loss 1.590884341812134\nIteration 13000: Loss 1.5990442565917968\nIteration 13100: Loss 1.6135609567642213\nIteration 13200: Loss 1.838941780090332\nIteration 13300: Loss 1.704717624282837\nIteration 13400: Loss 2.0010145835876463\nIteration 13500: Loss 1.6743137134552002\nIteration 13600: Loss 1.5059681663513182\nIteration 13700: Loss 1.6957318500518799\nIteration 13800: Loss 1.9573170822143555\nIteration 13900: Loss 1.5050548015594483\nIteration 14000: Loss 1.4584338151931762\nIteration 14100: Loss 1.7120291603088378\nIteration 14200: Loss 1.7227655158996582\nIteration 14300: Loss 1.5736059349060059\nIteration 14400: Loss 1.594474827194214\nIteration 14500: Loss 1.478431194114685\nIteration 14600: Loss 1.4531625797271728\nIteration 14700: Loss 1.5779596069335937\nIteration 14800: Loss 1.455890771484375\nIteration 14900: Loss 1.8466161960601806\nIteration 15000: Loss 1.5117101272583007\nIteration 15100: Loss 1.3834560285568238\nIteration 15200: Loss 1.7521124221801758\nIteration 15300: Loss 1.643199830341339\nIteration 15400: Loss 1.3873813716888428\nIteration 15500: Loss 1.4898222793579101\nIteration 15600: Loss 1.4347176586151122\nIteration 15700: Loss 1.7360410583496093\nIteration 15800: Loss 1.4073506359100343\nIteration 15900: Loss 1.3995953771591187\nIteration 16000: Loss 1.5431934051513672\nIteration 16100: Loss 1.7085426654815674\nIteration 16200: Loss 1.4831677043914795\nIteration 16300: Loss 1.4834259019851685\nIteration 16400: Loss 1.4759844841003418\nIteration 16500: Loss 1.7517361137390137\nIteration 16600: Loss 1.5429268146514892\nIteration 16700: Loss 1.4949825870513915\nIteration 16800: Loss 1.3403865180969239\nIteration 16900: Loss 1.785963153076172\nIteration 17000: Loss 1.4571145351409913\nIteration 17100: Loss 1.4971815345764161\nIteration 17200: Loss 1.3363316404342651\nIteration 17300: Loss 1.5616394981384278\nIteration 17400: Loss 1.5053422912597656\nIteration 17500: Loss 1.3193801069259643\nIteration 17600: Loss 1.3288881217956543\nIteration 17700: Loss 1.363479570198059\nIteration 17800: Loss 1.2868558059692383\nIteration 17900: Loss 1.6257368204116822\nIteration 18000: Loss 1.3746029985427857\nIteration 18100: Loss 1.2429505126953124\nIteration 18200: Loss 1.3245806732177734\nIteration 18300: Loss 1.3492373710632324\nIteration 18400: Loss 1.3034002787590027\nIteration 18500: Loss 1.4386666021347045\nIteration 18600: Loss 1.2656163385391235\nIteration 18700: Loss 1.4372117923736571\nIteration 18800: Loss 1.384098437309265\nIteration 18900: Loss 1.3663081047058105\nIteration 19000: Loss 1.2221866287231444\nIteration 19100: Loss 1.2436024084091186\nIteration 19200: Loss 1.2665597987890242\nIteration 19300: Loss 1.237940205001831\nIteration 19400: Loss 1.344134195804596\nIteration 19500: Loss 1.2914415069580079\nIteration 19600: Loss 1.3626669807434082\nIteration 19700: Loss 1.366358633518219\nIteration 19800: Loss 1.313393344116211\nIteration 19900: Loss 1.4002619796752929\nIteration 20000: Loss 1.294144804763794\nIteration 20100: Loss 1.1231573097229004\nIteration 20200: Loss 1.4601175102233888\nIteration 20300: Loss 1.3285247890472411\nIteration 20400: Loss 1.1695637263536454\nIteration 20500: Loss 1.2087780109405517\nIteration 20600: Loss 1.3476190698623658\nIteration 20700: Loss 1.2529712825775146\nIteration 20800: Loss 1.1856830729484558\nIteration 20900: Loss 1.4140738529205323\nIteration 21000: Loss 1.1243417324066163\nIteration 21100: Loss 1.3181340534210204\nIteration 21200: Loss 1.1900270936012267\nIteration 21300: Loss 1.2488590507507324\nIteration 21400: Loss 1.1325544092178346\nIteration 21500: Loss 1.3262826530456544\nIteration 21600: Loss 1.0915279739379882\nIteration 21700: Loss 1.2252526418685914\nIteration 21800: Loss 1.1583450969696045\nIteration 21900: Loss 1.082958779668808\nIteration 22000: Loss 1.1462610054016114\nIteration 22100: Loss 1.4945694793701172\nIteration 22200: Loss 1.1581610656738281\nIteration 22300: Loss 1.141967223072052\nIteration 22400: Loss 1.1137660387039185\nIteration 22500: Loss 1.175671806716919\nIteration 22600: Loss 1.4290738075256348\nIteration 22700: Loss 1.1575688766479493\nIteration 22800: Loss 1.0784797283172607\nIteration 22900: Loss 1.0384179958343507\nIteration 23000: Loss 1.224859179878235\nIteration 23100: Loss 1.0798201441764832\nIteration 23200: Loss 1.1970328790664673\nIteration 23300: Loss 1.0232666524648666\nIteration 23400: Loss 1.0703137565612793\nIteration 23500: Loss 1.1317445886611939\nIteration 23600: Loss 1.0022727109909058\nIteration 23700: Loss 1.2312337507247926\nIteration 23800: Loss 1.1835025502204894\nIteration 23900: Loss 1.0392642019271852\nIteration 24000: Loss 1.2640712344169616\nIteration 24100: Loss 1.8336103191375732\nIteration 24200: Loss 1.1796903217315673\nIteration 24300: Loss 1.0128382245063783\nIteration 24400: Loss 1.0055124671936035\nIteration 24500: Loss 1.1509688518047332\nIteration 24600: Loss 1.1975436107635498\nIteration 24700: Loss 1.0829024227142334\nIteration 24800: Loss 1.2921383535385131\nIteration 24900: Loss 1.201275763130188\nIteration 25000: Loss 1.0671045684814453\nIteration 25100: Loss 1.0469103141784668\nIteration 25200: Loss 1.2323695072174072\nIteration 25300: Loss 1.2606592887878418\nIteration 25400: Loss 1.4016080513000488\nIteration 25500: Loss 1.198599267578125\nIteration 25600: Loss 1.0564226654052735\nIteration 25700: Loss 1.0418739635467529\nIteration 25800: Loss 1.33170632686615\nIteration 25900: Loss 1.3476320137023925\nIteration 26000: Loss 1.0058715482711793\nIteration 26100: Loss 1.815998117828369\nIteration 26200: Loss 0.9853004442214965\nIteration 26300: Loss 0.9809446309089661\nIteration 26400: Loss 0.992795580148697\nIteration 26500: Loss 1.0417560279846192\nIteration 26600: Loss 0.9562216547489166\nIteration 26700: Loss 1.4398126748085023\nIteration 26800: Loss 1.0830552017211914\nIteration 26900: Loss 1.347120636177063\nIteration 27000: Loss 1.2469386428833007\nIteration 27100: Loss 1.2255584732055664\nIteration 27200: Loss 0.9324762265205383\nIteration 27300: Loss 1.0836722122192384\nIteration 27400: Loss 1.1341082429885865\nIteration 27500: Loss 1.0676711198806763\nIteration 27600: Loss 1.2318695472717285\nIteration 27700: Loss 0.9928106121063233\nIteration 27800: Loss 1.0703805303573608\nIteration 27900: Loss 1.1992874055862426\nIteration 28000: Loss 1.4026785018920898\nIteration 28100: Loss 1.6632488533973693\nIteration 28200: Loss 0.9016457536697388\nIteration 28300: Loss 1.0212578582763672\nIteration 28400: Loss 0.8860672996520996\nIteration 28500: Loss 0.8985200964927673\nIteration 28600: Loss 1.0489879613876343\nIteration 28700: Loss 1.0217605890274049\nIteration 28800: Loss 1.0733938774108887\nIteration 28900: Loss 1.0039743097305298\nIteration 29000: Loss 0.9304873789787292\nIteration 29100: Loss 1.0184979148864746\nIteration 29200: Loss 1.1306106702804566\nIteration 29300: Loss 0.9346900703430175\nIteration 29400: Loss 1.0740039623260498\nIteration 29500: Loss 0.9930446623563767\nIteration 29600: Loss 0.8834961042404175\nIteration 29700: Loss 1.0307254405975341\nIteration 29800: Loss 0.8264702445983887\nIteration 29900: Loss 1.1244774713516235\nIteration 30000: Loss 0.9604535228729248\nIteration 30100: Loss 0.9605722463607788\nIteration 30200: Loss 1.1951684909820557\nIteration 30300: Loss 0.9445425276756286\nIteration 30400: Loss 0.9402200061798096\nIteration 30500: Loss 1.0752807530403137\nIteration 30600: Loss 0.9742779733657837\nIteration 30700: Loss 0.9148640894889831\nIteration 30800: Loss 1.4603435163497924\nIteration 30900: Loss 1.0696831394195556\nIteration 31000: Loss 0.8406717923164367\nIteration 31100: Loss 1.2318050674438477\nIteration 31200: Loss 1.0886528155326842\nIteration 31300: Loss 1.2161739952087403\nIteration 31400: Loss 0.9475340072631836\nIteration 31500: Loss 0.876466389399767\nIteration 31600: Loss 0.8050463788032531\nIteration 31700: Loss 0.9721139904022217\nIteration 31800: Loss 0.9872968809127808\nIteration 31900: Loss 0.9398124222755432\nIteration 32000: Loss 1.0496617568969726\nIteration 32100: Loss 0.7003953777551651\nIteration 32200: Loss 0.686682488155365\nIteration 32300: Loss 0.6869098092079162\nIteration 32400: Loss 0.6917302596569062\nIteration 32500: Loss 0.6884131508827209\nIteration 32600: Loss 0.6870958906173706\nIteration 32700: Loss 0.6806457269668579\nIteration 32800: Loss 0.6869554368019104\nIteration 32900: Loss 0.68932054605484\nIteration 33000: Loss 0.694797169303894\nIteration 33100: Loss 0.6856534811019898\nIteration 33200: Loss 0.6942565475463868\nIteration 33300: Loss 0.6929962090492249\nIteration 33400: Loss 0.6992316924095153\nIteration 33500: Loss 0.7160737717628479\nIteration 33600: Loss 0.7179480255126953\nIteration 33700: Loss 0.7366914266586304\nIteration 33800: Loss 0.7470959331512451\nIteration 33900: Loss 0.7362483024597168\nIteration 34000: Loss 0.7397677616652101\nIteration 34100: Loss 0.7526388221740723\nIteration 34200: Loss 0.742811304473877\nIteration 34300: Loss 0.7523431972503662\nIteration 34400: Loss 0.735832909488678\nIteration 34500: Loss 0.7440882402420044\nIteration 34600: Loss 0.7576458916157484\nIteration 34700: Loss 0.7574836240768432\nIteration 34800: Loss 0.7518835892200469\nIteration 34900: Loss 0.7625729432582855\nIteration 35000: Loss 0.7557978276729583\nIteration 35100: Loss 0.7559810855865479\nIteration 35200: Loss 0.7560023907661438\nIteration 35300: Loss 0.7671687927246094\nIteration 35400: Loss 0.7814170000076294\nIteration 35500: Loss 0.7911865689754486\nIteration 35600: Loss 0.8102867163896561\nIteration 35700: Loss 0.8255473568916321\nIteration 35800: Loss 0.8012265586853027\nIteration 35900: Loss 0.8123232043266296\nIteration 36000: Loss 0.827883004951477\nIteration 36100: Loss 0.8126993385314941\nIteration 36200: Loss 0.8112236543655396\nIteration 36300: Loss 0.8256665344715118\nIteration 36400: Loss 0.8180895226478576\nIteration 36500: Loss 0.8211900973320008\nIteration 36600: Loss 0.8435358291625976\nIteration 36700: Loss 0.8228743396759033\nIteration 36800: Loss 0.8232708095550537\nIteration 36900: Loss 0.8278142397284508\nIteration 37000: Loss 0.825535809037619\nIteration 37100: Loss 0.8555444252967834\nIteration 37200: Loss 0.8680848180770874\nIteration 37300: Loss 0.8664574108123779\nIteration 37400: Loss 0.8833583860397339\nIteration 37500: Loss 0.8912087976455688\nIteration 37600: Loss 0.9243142364501953\nIteration 37700: Loss 0.9119870666742325\nIteration 37800: Loss 0.8944354505538941\nIteration 37900: Loss 0.9029494708269834\nIteration 38000: Loss 0.9297816844940185\nIteration 38100: Loss 0.9092510755538941\nIteration 38200: Loss 0.9196983306407929\nIteration 38300: Loss 0.900470294380188\nIteration 38400: Loss 0.9120138694763184\nIteration 38500: Loss 0.8893898866653442\nIteration 38600: Loss 0.8973721515655517\nIteration 38700: Loss 0.9039694026947022\nIteration 38800: Loss 0.9168472606182099\nIteration 38900: Loss 0.9535534458160401\nIteration 39000: Loss 0.9641849854946136\nIteration 39100: Loss 0.9931296043395996\nIteration 39200: Loss 1.0025251718521118\nIteration 39300: Loss 1.0060821767807007\nIteration 39400: Loss 0.9962821643829346\nIteration 39500: Loss 1.0191772666931151\nIteration 39600: Loss 0.9923745346069336\nIteration 39700: Loss 1.0210043382644653\nIteration 39800: Loss 0.9902482138633728\nIteration 39900: Loss 1.0159036518096924\nIteration 40000: Loss 1.0035509632110595\nIteration 40100: Loss 0.9926577596187591\nIteration 40200: Loss 0.9743582909584045\nIteration 40300: Loss 1.01192443857193\nIteration 40400: Loss 0.993764634513855\nIteration 40500: Loss 0.9958557739257813\nIteration 40600: Loss 1.0491375072479248\nIteration 40700: Loss 1.0499684722840785\nIteration 40800: Loss 1.0463620285034179\nIteration 40900: Loss 1.0834318117141724\nIteration 41000: Loss 1.1216295406341552\nIteration 41100: Loss 1.1015451860547065\nIteration 41200: Loss 1.0802704590797425\nIteration 41300: Loss 1.1162485630035401\nIteration 41400: Loss 1.1056290862083435\nIteration 41500: Loss 1.0897388216018677\nIteration 41600: Loss 1.0786601624175907\nIteration 41700: Loss 1.0866485441207885\nIteration 41800: Loss 1.0699220695495606\nIteration 41900: Loss 1.0675376781463624\nIteration 42000: Loss 1.0928807594299317\nIteration 42100: Loss 1.094549152481556\nIteration 42200: Loss 1.0551821222782134\nIteration 42300: Loss 1.1221625316619872\nIteration 42400: Loss 1.0980146348953248\nIteration 42500: Loss 1.1700675903320312\nIteration 42600: Loss 1.1308883271217347\nIteration 42700: Loss 1.1640855865478517\nIteration 42800: Loss 1.1410154494047164\nIteration 42900: Loss 1.1629783031463623\nIteration 43000: Loss 1.202889652121067\nIteration 43100: Loss 1.1651454985141754\nIteration 43200: Loss 1.1849644337654113\nIteration 43300: Loss 1.174847085762024\nIteration 43400: Loss 1.153751817512512\nIteration 43500: Loss 1.1769672996520997\nIteration 43600: Loss 1.1919489560127259\nIteration 43700: Loss 1.1279208039283752\nIteration 43800: Loss 1.1355155874252318\nIteration 43900: Loss 1.130147366809845\nIteration 44000: Loss 1.1512762811660766\nIteration 44100: Loss 1.1445326763153076\nIteration 44200: Loss 1.1478427015304566\nIteration 44300: Loss 1.223392145347595\nIteration 44400: Loss 1.2421758072853089\nIteration 44500: Loss 1.2263042466044427\nIteration 44600: Loss 1.256776560163498\nIteration 44700: Loss 1.255982134437561\nIteration 44800: Loss 1.2534911664023995\nIteration 44900: Loss 1.2492317421913146\nIteration 45000: Loss 1.2348642293453216\nIteration 45100: Loss 1.228381565475464\nIteration 45200: Loss 1.2250530483007431\nIteration 45300: Loss 1.2237140441894532\nIteration 45400: Loss 1.2514134155273438\nIteration 45500: Loss 1.17659819355011\nIteration 45600: Loss 1.1989813201904296\nIteration 45700: Loss 1.1684839780688285\nIteration 45800: Loss 1.2253613209724425\nIteration 45900: Loss 1.1942924478530883\nIteration 46000: Loss 1.2606334222044795\nIteration 46100: Loss 1.2446245193481444\nIteration 46200: Loss 1.2143155227661133\nIteration 46300: Loss 1.2415588598251344\nIteration 46400: Loss 1.1965966866016389\nIteration 46500: Loss 1.2802158226013183\nIteration 46600: Loss 1.2733156829833985\nIteration 46700: Loss 1.2235683034896851\nIteration 46800: Loss 1.251753359603882\nIteration 46900: Loss 1.2524628472328185\nIteration 47000: Loss 1.2450168154716492\nIteration 47100: Loss 1.2291645286560058\nIteration 47200: Loss 1.2133512924194336\nIteration 47300: Loss 1.247939102935791\nIteration 47400: Loss 1.2529980405807495\nIteration 47500: Loss 1.2152685299634933\nIteration 47600: Loss 1.2474958002090455\nIteration 47700: Loss 1.2640699924468994\nIteration 47800: Loss 1.2626136615753174\nIteration 47900: Loss 1.3243853452682495\nIteration 48000: Loss 1.2924530450820924\nIteration 48100: Loss 1.2401840348005295\nIteration 48200: Loss 1.2394668628692627\nIteration 48300: Loss 1.2434401672363282\nIteration 48400: Loss 1.2409987957000732\nIteration 48500: Loss 1.2298211217880248\nIteration 48600: Loss 1.233865307855606\nIteration 48700: Loss 1.2457696044921875\nIteration 48800: Loss 1.2461982326507568\nIteration 48900: Loss 1.2443075534820556\nIteration 49000: Loss 1.2478720040082931\nIteration 49100: Loss 1.255149599170685\nIteration 49200: Loss 1.2522606800079346\nIteration 49300: Loss 1.2611784803390502\nIteration 49400: Loss 1.262726113319397\nIteration 49500: Loss 1.2571825397491454\nIteration 49600: Loss 1.2740822111442685\nIteration 49700: Loss 1.2795643226623534\nIteration 49800: Loss 1.2696777786254883\nIteration 49900: Loss 1.278622400665283\nIteration 50000: Loss 1.2765386125564575\nIteration 50100: Loss 1.281493995666504\nIteration 50200: Loss 1.282387308883667\nIteration 50300: Loss 1.2835626153945923\nIteration 50400: Loss 1.2859542266845703\nIteration 50500: Loss 1.2892499279022216\nIteration 50600: Loss 1.2847719497680663\nIteration 50700: Loss 1.2943070127487182\nIteration 50800: Loss 1.2896253036499024\nIteration 50900: Loss 1.2871786117553712\nIteration 51000: Loss 1.2984485795667395\nIteration 51100: Loss 1.2874908638000488\nIteration 51200: Loss 1.2898776433944703\nIteration 51300: Loss 1.2985285400390625\nIteration 51400: Loss 1.302291520500183\nIteration 51500: Loss 1.307137019112706\nIteration 51600: Loss 1.3089366313934325\nIteration 51700: Loss 1.318252717590332\nIteration 51800: Loss 1.3181390121936798\nIteration 51900: Loss 1.3103392761230468\nIteration 52000: Loss 1.3121946014404298\nIteration 52100: Loss 1.31045973072052\nIteration 52200: Loss 1.3106559768676758\nIteration 52300: Loss 1.3131308528900147\nIteration 52400: Loss 1.3159589500427247\nIteration 52500: Loss 1.310640513420105\nIteration 52600: Loss 1.3185210545778274\nIteration 52700: Loss 1.3192638946533204\nIteration 52800: Loss 1.3205116841048001\nIteration 52900: Loss 1.3368839290142058\nIteration 53000: Loss 1.3368506813049317\nIteration 53100: Loss 1.3293768663406371\nIteration 53200: Loss 1.3333360056877137\nIteration 53300: Loss 1.3357858769416808\nIteration 53400: Loss 1.3350441246986389\nIteration 53500: Loss 1.3423253253936767\nIteration 53600: Loss 1.334006096148491\nIteration 53700: Loss 1.3446114370942115\nIteration 53800: Loss 1.342708822631836\nIteration 53900: Loss 1.3488070140838624\nIteration 54000: Loss 1.3455445508778094\nIteration 54100: Loss 1.3476375988006593\nIteration 54200: Loss 1.3507031072616578\nIteration 54300: Loss 1.3502052909851074\nIteration 54400: Loss 1.366765460205078\nIteration 54500: Loss 1.3503408834457398\nIteration 54600: Loss 1.3586708240509033\nIteration 54700: Loss 1.3534940301895142\nIteration 54800: Loss 1.3585821674346923\nIteration 54900: Loss 1.3635480308532715\nIteration 55000: Loss 1.3547804244995116\nIteration 55100: Loss 1.36165051612854\nIteration 55200: Loss 1.3573915105819703\nIteration 55300: Loss 1.3514466936111451\nIteration 55400: Loss 1.3738964080810547\nIteration 55500: Loss 1.3687130510751158\nIteration 55600: Loss 1.367083456516266\nIteration 55700: Loss 1.3728675952911378\nIteration 55800: Loss 1.3716580429077148\nIteration 55900: Loss 1.3670056678771974\nIteration 56000: Loss 1.3670362564086913\nIteration 56100: Loss 1.3758971920490264\nIteration 56200: Loss 1.3741923309326172\nIteration 56300: Loss 1.3721398376464844\nIteration 56400: Loss 1.3703158406734466\nIteration 56500: Loss 1.3798056056976318\nIteration 56600: Loss 1.3799421070098876\nIteration 56700: Loss 1.3805686235427856\nIteration 56800: Loss 1.3877108898162842\nIteration 56900: Loss 1.379222283554077\nIteration 57000: Loss 1.3852569657325744\nIteration 57100: Loss 1.3855635538578033\nIteration 57200: Loss 1.3868566478729247\nIteration 57300: Loss 1.3869994247436523\nIteration 57400: Loss 1.3840740603446962\nIteration 57500: Loss 1.396026176261902\nIteration 57600: Loss 1.3866209686279296\nIteration 57700: Loss 1.3900431455612183\nIteration 57800: Loss 1.3989044837832452\nIteration 57900: Loss 1.3822583716392518\nIteration 58000: Loss 1.3935281955718994\nIteration 58100: Loss 1.4186854028701783\nIteration 58200: Loss 1.400551718044281\nIteration 58300: Loss 1.4025471658706665\nIteration 58400: Loss 1.4023237775802613\nIteration 58500: Loss 1.4044990413665772\nIteration 58600: Loss 1.4062872787475587\nIteration 58700: Loss 1.407593269252777\nIteration 58800: Loss 1.4147026165008545\nIteration 58900: Loss 1.4108387210845947\nIteration 59000: Loss 1.413429334640503\nIteration 59100: Loss 1.4088375490188598\nIteration 59200: Loss 1.415093978023529\nIteration 59300: Loss 1.408053803730011\nIteration 59400: Loss 1.4172991102218628\nIteration 59500: Loss 1.4166585792541504\nIteration 59600: Loss 1.4152519590377808\nIteration 59700: Loss 1.4155409870147706\nIteration 59800: Loss 1.422481104660034\nIteration 59900: Loss 1.4243515510559082\nIteration 60000: Loss 1.4193086753845214\nIteration 60100: Loss 1.4264763975143433\nIteration 60200: Loss 1.4207589290618896\nIteration 60300: Loss 1.4249505462765695\nIteration 60400: Loss 1.4268912071466446\nIteration 60500: Loss 1.4348493120193482\nIteration 60600: Loss 1.4229291135385633\nIteration 60700: Loss 1.4301045207977294\nIteration 60800: Loss 1.4303565311431885\nIteration 60900: Loss 1.4348065597534179\nIteration 61000: Loss 1.4250145637512206\nIteration 61100: Loss 1.4293483751296998\nIteration 61200: Loss 1.436556676673889\nIteration 61300: Loss 1.4329486372947693\nIteration 61400: Loss 1.447090312576294\nIteration 61500: Loss 1.4406044860839844\nIteration 61600: Loss 1.4383559973716735\nIteration 61700: Loss 1.4294200409889222\nIteration 61800: Loss 1.4369571266174317\nIteration 61900: Loss 1.4430343266487122\nIteration 62000: Loss 1.4450821937561036\nIteration 62100: Loss 1.4371973932266235\nIteration 62200: Loss 1.428347486114502\nIteration 62300: Loss 1.4428495470046998\nIteration 62400: Loss 1.4536977517235092\nIteration 62500: Loss 1.447822782356292\nIteration 62600: Loss 1.4472046253204345\nIteration 62700: Loss 1.444363646697998\nIteration 62800: Loss 1.4543246311187745\nIteration 62900: Loss 1.45516748046875\nIteration 63000: Loss 1.4579125593185425\nIteration 63100: Loss 1.4426148216247558\nIteration 63200: Loss 1.4578731605529784\nIteration 63300: Loss 1.4558767906188965\nIteration 63400: Loss 1.4600901123046874\nIteration 63500: Loss 1.458589810180664\nIteration 63600: Loss 1.4593489006042482\nIteration 63700: Loss 1.458819625854492\nIteration 63800: Loss 1.4537692197799683\nIteration 63900: Loss 1.447675830078125\nIteration 64000: Loss 1.4477576036453248\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"model.eval()\ntest_loader = DataLoader([*zip(test_images, test_labels)], batch_size=batch_size, shuffle=False)\ncorrect = 0\nfor batch in test_loader:\n  features, target = batch[:-1], batch[-1]\n  features = features[0].to(device)\n  target = target.to(device)\n  outputs = model(features)\n  correct += torch.where(torch.argmax(torch.softmax(outputs, dim=1), dim=1)==target, 1, 0).sum()\nprint((correct/test_images.shape[0])*100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T08:17:02.189036Z","iopub.execute_input":"2025-05-13T08:17:02.189365Z","iopub.status.idle":"2025-05-13T08:17:04.690937Z","shell.execute_reply.started":"2025-05-13T08:17:02.189348Z","shell.execute_reply":"2025-05-13T08:17:04.690329Z"}},"outputs":[{"name":"stdout","text":"tensor(76.2400, device='cuda:0')\n","output_type":"stream"}],"execution_count":19}]}